"""
Document Summarizer Lambda
Combines all segment content_combined to generate document-level summary using LLM
"""

import json
import logging
import os
from typing import Dict, Any, List
from datetime import datetime, timezone
from botocore.exceptions import ClientError

# Common module imports
from common import (
    DynamoDBService,
    handle_lambda_error,
    get_current_timestamp
)

# Logging setup
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# Initialize common services
db_service = DynamoDBService()


class SkipSummaryError(Exception):
    """Raised when summary generation should be skipped (e.g., context limit)."""
    pass

def lambda_handler(event: Dict[str, Any], context) -> Dict[str, Any]:
    """
    Document Summarizer Lambda handler
    Generate document-level summary from all segment content_combined using LLM
    
    Args:
        event: Event received from Step Function (includes document_id and index_id)
        context: Lambda context
        
    Returns:
        Processing result
    """
    try:
        logger.info(f"Document Summarizer started - Event: {json.dumps(event, ensure_ascii=False, indent=2)}")
        
        # Extract input data
        index_id = event.get('index_id')
        document_id = event.get('document_id')
        
        if not document_id or not index_id:
            raise ValueError("Both document_id and index_id are required.")
        
        logger.info(f"ğŸ“‹ Target: index_id={index_id}, document_id={document_id}")
        
        # Mark document as summarizing in-progress (status only)
        try:
            current_time = get_current_timestamp()
            db_service.update_item(
                table_name='documents',
                key={'document_id': document_id},
                update_expression='SET #status = :status, updated_at = :updated_at',
                expression_attribute_names={'#status': 'status'},
                expression_attribute_values={
                    ':status': 'summarizing',
                    ':updated_at': current_time
                }
            )
            logger.info(f"ğŸ“Œ Document status set to summarizing: {document_id}")
        except Exception as se:
            logger.warning(f"âš ï¸ Failed to set summarizing status: {str(se)}")

        # Generate document summary
        summary_result = generate_document_summary(document_id, index_id)
        
        if summary_result:
            if summary_result.get('skipped'):
                logger.warning(f"âš ï¸ Document summary skipped due to context limit: {document_id}")
            else:
                logger.info(f"âœ… Document summary generation complete: {document_id}")
            
            # Update document status to final completion
            update_document_final_status(document_id, True)
            
            return {
                'success': True,
                'message': 'Document summary skipped due to context limit' if summary_result.get('skipped') else 'Document summary generation complete',
                'document_id': document_id,
                'index_id': index_id,
                'summary_length': len(summary_result.get('summary', '')),
                'skipped': summary_result.get('skipped', False)
            }
        else:
            logger.error(f"âŒ Document summary generation failed: {document_id}")
            
            # Update document status to indicate failure
            update_document_final_status(document_id, False)
            
            return {
                'success': False,
                'message': 'Document summary generation failed',
                'document_id': document_id,
                'index_id': index_id
            }
        
    except Exception as e:
        logger.error(f"Document Summarizer error: {str(e)}", exc_info=True)
        return handle_lambda_error(e)

def generate_document_summary(document_id: str, index_id: str) -> Dict[str, Any]:
    """Generate document-level summary from all segments"""
    try:
        logger.info(f"ğŸ“„ Document summary generation started: {document_id}")
        
        # Get document information
        document = db_service.get_item('documents', {'document_id': document_id})
        if not document:
            logger.error(f"âŒ Document not found: {document_id}")
            return None
        
        media_type = document.get('media_type', 'DOCUMENT')
        file_name = document.get('file_name', 'Unknown')
        
        logger.info(f"ğŸ“„ Document info: {file_name} ({media_type})")
        
        # Retrieve all segments for the document
        from boto3.dynamodb.conditions import Key
        segments_response = db_service.query_items(
            table_name='segments',
            key_condition_expression=Key('document_id').eq(document_id),
            index_name='DocumentIdIndex'
        )
        
        segments = segments_response.get('Items', [])
        logger.info(f"ğŸ“‹ Total segments found: {len(segments)}")
        
        if not segments:
            logger.warning(f"âš ï¸ No segments found for document: {document_id}")
            return None
        
        # Collect per-segment short summaries (3~4 lines) using Haiku, then combine
        segment_contents = []
        
        for segment in segments:
            segment_id = segment.get('segment_id')
            segment_index = segment.get('segment_index', 0)
            segment_type = segment.get('segment_type', 'PAGE')
            
            if not segment_id:
                continue

            # Prefer existing content fields as source; we'll re-summarize to 3~4 lines using Haiku
            source_text = (
                segment.get('content_combined')
                or segment.get('summary')
                or segment.get('content')
                or ''
            )

            short_summary = ''
            if isinstance(source_text, str) and source_text.strip():
                try:
                    short_summary = generate_page_summary_with_llm(
                        source_text.strip(),
                        media_type,
                        file_name,
                        segment_index,
                    )
                except Exception as e:
                    logger.warning(f"âš ï¸ Haiku page summary failed for segment {segment_index}: {str(e)}")

            # Fallback if summarization failed
            if not short_summary:
                fallback_text = (segment.get('summary') or segment.get('content_combined') or '')
                short_summary = (fallback_text[:800] + '...') if isinstance(fallback_text, str) else ''

            if short_summary:
                segment_contents.append({
                    'segment_index': segment_index,
                    'segment_type': segment_type,
                    'content': short_summary.strip()
                })
                logger.debug(f"ğŸ“„ Segment {segment_index} short summary collected: {len(short_summary)} chars")
        
        if not segment_contents:
            logger.warning(f"âš ï¸ No segment summaries found for document: {document_id}")
            return None
        
        # Sort by segment_index
        segment_contents.sort(key=lambda x: x['segment_index'])
        
        logger.info(f"ğŸ“Š Total segments with summaries: {len(segment_contents)}")
        
        # Generate combined content from short summaries
        combined_content = create_combined_content_for_llm(segment_contents, media_type, file_name)
        
        # Generate summary using LLM
        summary_skipped = False
        try:
            summary = generate_summary_with_llm(combined_content, media_type, file_name)
        except SkipSummaryError as se:
            # Context limit exceeded (input length + max_tokens)
            logger.warning(f"âš ï¸ Skipping summary generation due to context limit: {str(se)}")
            summary = ""
            summary_skipped = True
        
        if summary_skipped:
            # Treat as success without updating summary
            return {
                'summary': summary,
                'segment_count': len(segment_contents),
                'total_content_length': sum(len(s['content']) for s in segment_contents),
                'skipped': True
            }

        if summary:
            # Update Documents table with summary
            success = update_document_summary(document_id, summary)
            
            if success:
                return {
                    'summary': summary,
                    'segment_count': len(segment_contents),
                    'total_content_length': sum(len(s['content']) for s in segment_contents),
                    'skipped': False
                }
        
        return None
        
    except Exception as e:
        logger.error(f"âŒ Document summary generation error: {str(e)}")
        raise

def create_combined_content_for_llm(segment_contents: List[Dict[str, Any]], media_type: str, file_name: str) -> str:
    """Create combined content for LLM input"""
    try:
        content_parts = []
        
        # Add header based on media type
        if media_type == 'VIDEO':
            content_parts.append(f"# ë™ì˜ìƒ '{file_name}' ì „ì²´ ë¶„ì„ ë‚´ìš©\n")
            content_parts.append(f"ë‹¤ìŒì€ {len(segment_contents)}ê°œ ì±•í„°ë³„ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤:\n\n")
        else:
            content_parts.append(f"# ë¬¸ì„œ '{file_name}' ì „ì²´ ë¶„ì„ ë‚´ìš©\n")
            content_parts.append(f"ë‹¤ìŒì€ {len(segment_contents)}ê°œ í˜ì´ì§€ë³„ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤:\n\n")
        
        # Add each segment's content (prefer summary-based 'content')
        for segment in segment_contents:
            segment_index = segment['segment_index']
            segment_type = segment['segment_type']
            # Prefer 'content' (segment summary). Fallback to 'content_combined' for backward compatibility.
            content_text = segment.get('content') or segment.get('content_combined') or ""
            
            if media_type == 'VIDEO':
                content_parts.append(f"## ì±•í„° {segment_index + 1}\n")
            else:
                content_parts.append(f"## í˜ì´ì§€ {segment_index + 1}\n")
            
            content_parts.append(f"{content_text}\n\n")
        
        combined_content = "\n".join(content_parts)
        
        logger.info(f"ğŸ“ Combined content for LLM: {len(combined_content)} chars")
        return combined_content
        
    except Exception as e:
        logger.error(f"âŒ Error creating combined content: {str(e)}")
        return ""

def generate_summary_with_llm(content: str, media_type: str, file_name: str) -> str:
    """Generate summary using LLM (Bedrock)"""
    try:
        import boto3
        
        # Get Bedrock configuration
        model_id = os.environ.get('BEDROCK_SUMMARY_MODEL_ID', 'us.anthropic.claude-3-7-sonnet-20250219-v1:0')
        max_tokens = int(os.environ.get('BEDROCK_SUMMARY_MAX_TOKENS', '64000'))
        
        bedrock_runtime = boto3.client('bedrock-runtime')
        
        # Create prompt based on media type
        if media_type == 'VIDEO':
            system_prompt = f"""ë‹¹ì‹ ì€ ë™ì˜ìƒ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ë™ì˜ìƒ '{file_name}'ì˜ ì±•í„°ë³„ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „ì²´ ë™ì˜ìƒì˜ ì¢…í•©ì ì¸ ìš”ì•½ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

ìš”ì•½ì€ ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:
1. ë™ì˜ìƒ ì „ì²´ ê°œìš” (ì£¼ì œ, ëª©ì , ëŒ€ìƒ ë“±)
2. ì£¼ìš” ë‚´ìš© ë° í•µì‹¬ ë©”ì‹œì§€
3. ì±•í„°ë³„ ìš”ì  ì •ë¦¬
4. ê²°ë¡  ë° ì‹œì²­ìì—ê²Œ ì „ë‹¬í•˜ê³ ì í•˜ëŠ” ë©”ì‹œì§€

ì „ë¬¸ì ì´ê³  ì²´ê³„ì ìœ¼ë¡œ ì‘ì„±í•˜ë˜, ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”."""

            user_prompt = f"ë‹¤ìŒì€ ë™ì˜ìƒ '{file_name}'ì˜ ì±•í„°ë³„ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „ì²´ ë™ì˜ìƒì˜ ì¢…í•© ìš”ì•½ì„ ìƒì„±í•´ì£¼ì„¸ìš”:\n\n{content}"
        else:
            system_prompt = f"""ë‹¹ì‹ ì€ ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œ '{file_name}'ì˜ í˜ì´ì§€ë³„ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „ì²´ ë¬¸ì„œì˜ ì¢…í•©ì ì¸ ìš”ì•½ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

ìš”ì•½ì€ ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:
1. ë¬¸ì„œ ì „ì²´ ê°œìš” (ì£¼ì œ, ëª©ì , ëŒ€ìƒ ë“±)
2. ì£¼ìš” ë‚´ìš© ë° í•µì‹¬ í¬ì¸íŠ¸
3. í˜ì´ì§€ë³„ ì¤‘ìš” ì‚¬í•­ ì •ë¦¬
4. ê²°ë¡  ë° ë¬¸ì„œì˜ í•µì‹¬ ë©”ì‹œì§€

ì „ë¬¸ì ì´ê³  ì²´ê³„ì ìœ¼ë¡œ ì‘ì„±í•˜ë˜, ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”."""

            user_prompt = f"ë‹¤ìŒì€ ë¬¸ì„œ '{file_name}'ì˜ í˜ì´ì§€ë³„ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „ì²´ ë¬¸ì„œì˜ ì¢…í•© ìš”ì•½ì„ ìƒì„±í•´ì£¼ì„¸ìš”:\n\n{content}"
        
        # Prepare request body
        request_body = {
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": max_tokens,
            "system": system_prompt,
            "messages": [
                {
                    "role": "user",
                    "content": user_prompt
                }
            ]
        }
        
        # Call Bedrock
        logger.info(f"ğŸ¤– Calling Bedrock model: {model_id}")
        try:
            response = bedrock_runtime.invoke_model(
                modelId=model_id,
                body=json.dumps(request_body)
            )
        except ClientError as ce:
            code = (ce.response.get('Error') or {}).get('Code')
            message = (ce.response.get('Error') or {}).get('Message', '')
            # Detect context limit overflow and signal skip
            if code == 'ValidationException' and 'context limit' in message:
                raise SkipSummaryError(message)
            raise

        # Parse response
        response_body = json.loads(response['body'].read())
        summary = response_body['content'][0]['text']
        
        logger.info(f"âœ… LLM summary generated: {len(summary)} chars")
        return summary
        
    except SkipSummaryError:
        # Bubble up to caller to handle as "skipped"
        raise
    except Exception as e:
        logger.error(f"âŒ LLM summary generation error: {str(e)}")
        return ""

def generate_page_summary_with_llm(content: str, media_type: str, file_name: str, segment_index: int) -> str:
    """Generate 3~4 line page/chapter summary using Haiku"""
    try:
        import boto3

        model_id = os.environ.get('BEDROCK_PAGE_SUMMARY_MODEL_ID', 'us.anthropic.claude-3-5-haiku-20241022-v1:0')
        max_tokens_env = int(os.environ.get('BEDROCK_PAGE_SUMMARY_MAX_TOKENS', '8192'))
        # Cap to a reasonable output length for 3~4 lines
        max_tokens = min(max_tokens_env, 512)

        bedrock_runtime = boto3.client('bedrock-runtime')

        if media_type == 'VIDEO':
            system_prompt = (
                "ë‹¹ì‹ ì€ ë™ì˜ìƒ ì±•í„° ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì…ë ¥ëœ ì±•í„° ë‚´ìš©ì„ 3~4ì¤„ì˜ í•œêµ­ì–´ ìš”ì•½ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì •ë¦¬í•˜ì„¸ìš”. "
                "ë¶ˆí•„ìš”í•œ ì„œë¡ /ê²°ë¡  ì—†ì´ í•µì‹¬ë§Œ ë‹´ê³ , ë¬¸ì¥ì€ ì§§ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”."
            )
            user_prompt = f"ë™ì˜ìƒ '{file_name}'ì˜ ì±•í„° {segment_index + 1} ë‚´ìš©ì…ë‹ˆë‹¤. 3~4ì¤„ ìš”ì•½:\n\n{content}"
        else:
            system_prompt = (
                "ë‹¹ì‹ ì€ ë¬¸ì„œ í˜ì´ì§€ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì…ë ¥ëœ í˜ì´ì§€ ë‚´ìš©ì„ 3~4ì¤„ì˜ í•œêµ­ì–´ ìš”ì•½ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì •ë¦¬í•˜ì„¸ìš”. "
                "ë¶ˆí•„ìš”í•œ ì„œë¡ /ê²°ë¡  ì—†ì´ í•µì‹¬ë§Œ ë‹´ê³ , ë¬¸ì¥ì€ ì§§ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”."
            )
            user_prompt = f"ë¬¸ì„œ '{file_name}'ì˜ í˜ì´ì§€ {segment_index + 1} ë‚´ìš©ì…ë‹ˆë‹¤. 3~4ì¤„ ìš”ì•½:\n\n{content}"

        request_body = {
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": max_tokens,
            "system": system_prompt,
            "messages": [
                {
                    "role": "user",
                    "content": user_prompt,
                }
            ]
        }

        logger.debug(f"ğŸ¤– Calling Haiku for page {segment_index + 1}: {model_id}")
        response = bedrock_runtime.invoke_model(
            modelId=model_id,
            body=json.dumps(request_body)
        )

        response_body = json.loads(response['body'].read())
        page_summary = response_body['content'][0]['text']
        return page_summary

    except Exception as e:
        logger.error(f"âŒ Page-level LLM summary error: {str(e)}")
        return ""

def update_document_summary(document_id: str, summary: str) -> bool:
    """Update Documents table with generated summary"""
    try:
        current_time = get_current_timestamp()
        
        update_response = db_service.update_item(
            table_name='documents',
            key={'document_id': document_id},
            update_expression='SET summary = :summary, updated_at = :updated_at',
            expression_attribute_values={
                ':summary': summary,
                ':updated_at': current_time
            }
        )
        
        logger.info(f"ğŸ“„ Document summary updated: {document_id} ({len(summary)} chars)")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Document summary update failed: {str(e)}")
        return False

def update_document_final_status(document_id: str, success: bool) -> None:
    """Update document status to final completion"""
    try:
        current_time = get_current_timestamp()
        final_status = 'completed' if success else 'summary_failed'
        
        update_response = db_service.update_item(
            table_name='documents',
            key={'document_id': document_id},
            update_expression='SET #status = :status, updated_at = :updated_at',
            expression_attribute_names={'#status': 'status'},
            expression_attribute_values={
                ':status': final_status,
                ':updated_at': current_time
            }
        )
        
        logger.info(f"ğŸ“„ Document final status update: {document_id} -> {final_status}")
        
    except Exception as e:
        logger.error(f"âŒ Document status update failed: {str(e)}")

# í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜
if __name__ == "__main__":
    # ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©
    test_event = {
        "index_id": "test_index_123",
        "document_id": "test_document_456"
    }
    
    result = lambda_handler(test_event, None)
    print(json.dumps(result, ensure_ascii=False, indent=2))
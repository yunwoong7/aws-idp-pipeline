"""
Refactored AnalysisAgent - Optimized for LangGraph structure
"""

import logging
import time
from datetime import datetime, timezone
from typing import Dict, Any
from langchain_core.messages import HumanMessage
from langchain_core.runnables import RunnableConfig

# Common module imports
import sys
import os
sys.path.append('/opt/python')
from common import OpenSearchService
from common.dynamodb_service import DynamoDBService

from agent.llm import get_llm
from agent.state.agent_state import AgentState
from agent.graph.builder import create_analysis_graph

logger = logging.getLogger(__name__)


class AnalysisAgent:
    """
    LangGraph-based analysis agent
    """
    
    def __init__(self, 
                 index_id: str = None,
                 document_id: str = None,
                 segment_id: str = None,
                 segment_index: int = None,
                 image_uri: str = None,
                 file_path: str = None,
                 model_id: str = None,
                 max_tokens: int = 8192,
                 thread_id: str = None,
                 segment_type: str = None,
                 start_timecode_smpte: str = None,
                 end_timecode_smpte: str = None,
                 media_type: str = None):
        
        logger.info("ğŸš€ AnalysisAgent initialization started")
        
        # Set basic information
        self.index_id = index_id
        self.document_id = document_id
        self.segment_id = segment_id
        self.segment_index = segment_index
        self.image_uri = image_uri
        self.file_path = file_path
        
        # ë™ì˜ìƒ ì±•í„° ì •ë³´ (VideoAnalyzerToolìš©)
        self.segment_type = segment_type
        self.start_timecode_smpte = start_timecode_smpte
        self.end_timecode_smpte = end_timecode_smpte
        
        # ë¬¸ì„œ íƒ€ì… ì •ë³´
        self.media_type = media_type or 'DOCUMENT'
        
        # Set thread_id for conversation continuity
        self.thread_id = thread_id or f"document_{document_id}_default"
        logger.info(f"ğŸ”„ Thread ID set to: {self.thread_id}")
        
        # Initialize LLM model
        self.model = get_llm(model_id=model_id, max_tokens=max_tokens)
        
        # Initialize OpenSearch service (for previous analysis lookup)
        self.opensearch_service = self._init_opensearch()
        
        # Initialize DynamoDB service (for updating segments table)
        self.dynamodb_service = self._init_dynamodb()
        
        # Create LangGraph
        self.graph = create_analysis_graph(self.model)
        
        logger.info(f"âœ… AnalysisAgent initialization completed")
        logger.info(f"ğŸ“ Document: {document_id}, Segment: {segment_index}")

    
    def _init_opensearch(self):
        """Initialize OpenSearch service"""
        opensearch_endpoint = os.environ.get('OPENSEARCH_ENDPOINT')
        if not opensearch_endpoint:
            logger.warning("âš ï¸ OpenSearch endpoint is not set")
            return None
        
        try:
            service = OpenSearchService(
                endpoint=opensearch_endpoint,
                index_name=os.environ.get('OPENSEARCH_INDEX_NAME', 'aws-idp-ai-analysis'),
                region=os.environ.get('OPENSEARCH_REGION') or os.environ.get('AWS_REGION', 'us-west-2')
            )
            logger.info("âœ… OpenSearch service initialization completed")
            return service
        except Exception as e:
            logger.warning(f"âŒ OpenSearch initialization failed: {str(e)}")
            return None
    
    def _init_dynamodb(self):
        """Initialize DynamoDB service"""
        try:
            service = DynamoDBService(region=os.environ.get('AWS_REGION', 'us-west-2'))
            logger.info("âœ… DynamoDB service initialization completed")
            return service
        except Exception as e:
            logger.warning(f"âŒ DynamoDB initialization failed: {str(e)}")
            return None
    
    def analyze_document(self, user_query: str = None, analysis_type: str = "comprehensive") -> Dict[str, Any]:
        """
        Execute document analysis
        
        Args:
            user_query: User query
            analysis_type: Analysis type
            
        Returns:
            Analysis result
        """
        logger.info("-" * 50)
        logger.info("ğŸ¯ Document analysis started")
        logger.info("-" * 50)
        logger.info(f"ğŸ’¬ Analysis query: {user_query}")
        logger.info(f"ğŸ“‹ Analysis type: {analysis_type}")
        
        start_time = time.time()
        
        try:
            # 1. Get previous analysis context
            previous_analysis_context = self._get_previous_analysis()
            
            # 2. Create initial state
            initial_state = self._create_initial_state(user_query, previous_analysis_context)
            
            # 3. Create execution configuration
            config = self._create_config()
            
            # 4. Execute graph
            logger.info("ğŸš€ _execute_graph í˜¸ì¶œ ì‹œì‘")
            final_state = self._execute_graph(initial_state, config)
            logger.info("âœ… _execute_graph ì™„ë£Œ")
            
            # 5. Process results
            logger.info("ğŸš€ _process_results í˜¸ì¶œ ì‹œì‘")
            result = self._process_results(final_state, start_time)
            logger.info("âœ… _process_results ì™„ë£Œ")
            
            logger.info("-" * 50)
            logger.info("ğŸ‰ Document analysis completed")
            logger.info("-" * 50)
            
            return result
            
        except Exception as e:
            error_msg = f"Document analysis execution failed: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            import traceback
            logger.error(f"âŒ ì˜ˆì™¸ ìƒì„¸: {traceback.format_exc()}")
            
            return {
                'success': False,
                'error': str(e),
                'document_id': self.document_id,
                'timestamp': datetime.now(timezone.utc).isoformat()
            }
    
    def _get_previous_analysis(self) -> str:
        """Get previous analysis context"""
        if not self.opensearch_service:
            logger.info("ğŸ“‹ OpenSearch disabled - no previous analysis")
            return "**Previous analysis context**: OpenSearch disabled"
        
        try:
            logger.info("1. Get previous analysis context")
            
            filters = {
                "document_id": self.document_id,
                "segment_id": self.segment_id
            }
            
            response = self.opensearch_service.search_text(
                index_id=self.index_id,
                query="*",
                size=100,
                filters=filters
            )
            
            hits = response.get('hits', {}).get('hits', [])
            
            if not hits:
                logger.info("ğŸ“‹ No previous analysis results")
                return "**Previous analysis context**: No previous analysis results"
            
            # Construct analysis content
            context_parts = [f"**Previous analysis context** ({len(hits)} results):"]
            
            for i, hit in enumerate(hits, 1):
                source = hit.get('_source', {})
                
                # Extract actual content from new segment-unit structure
                content = self._extract_content_from_source(source)
                tool_name = self._extract_tool_name_from_source(source)
                created_at = source.get('created_at', '')
                
                if content and content.strip():
                    # If content is too long, summarize (use environment variable)
                    max_chars = int(os.environ.get('PREVIOUS_ANALYSIS_MAX_CHARACTERS', '100000'))
                    if len(content) > max_chars:
                        content_preview = content[:max_chars] + "...[summarized]"
                    else:
                        content_preview = content
                    
                    context_parts.append(f"\n{i}. **{tool_name}** ({created_at})")
                    context_parts.append(f"   {content_preview}")
            
            result = "\n".join(context_parts)
            logger.info(f"âœ… Previous analysis context lookup completed: {len(result)} characters")
            
            # Previous analysis context preview
            logger.info(f"ğŸ“‹ Previous analysis context preview:\n{result}")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ Previous analysis context lookup failed: {str(e)}")
            return f"**Previous analysis context**: Lookup failed ({str(e)})"
    
    def _extract_content_from_source(self, source: Dict[str, Any]) -> str:
        """Extract actual analysis content from OpenSearch source"""
        content_parts = []
        
        # 1. content_combined field (highest priority)
        content_combined = source.get('content_combined', '')
        if content_combined and content_combined.strip():
            content_parts.append(content_combined)
        
        # 2. Extract content from tools structure
        tools = source.get('tools', {})
        
        # bda_indexer tool content
        bda_tools = tools.get('bda_indexer', [])
        for bda_tool in bda_tools:
            if isinstance(bda_tool, dict):
                bda_content = bda_tool.get('content', '')
                if bda_content and bda_content.strip():
                    content_parts.append(f"[BDA ë¶„ì„] {bda_content}")
        
        # pdf_text_extractor tool content
        pdf_tools = tools.get('pdf_text_extractor', [])
        for pdf_tool in pdf_tools:
            if isinstance(pdf_tool, dict):
                pdf_content = pdf_tool.get('content', '')
                if pdf_content and pdf_content.strip():
                    content_parts.append(f"[PDF í…ìŠ¤íŠ¸] {pdf_content}")
        
        # image_analysis tool content
        img_tools = tools.get('image_analysis', [])
        for img_tool in img_tools:
            if isinstance(img_tool, dict):
                img_content = img_tool.get('content', '')
                if img_content and img_content.strip():
                    content_parts.append(f"[ì´ë¯¸ì§€ ë¶„ì„] {img_content}")
        
        # 3. Legacy content field (fallback)
        legacy_content = source.get('content', '')
        if legacy_content and legacy_content.strip() and not content_parts:
            content_parts.append(legacy_content)
        
        return '\n\n'.join(content_parts) if content_parts else ''
    
    def _extract_tool_name_from_source(self, source: Dict[str, Any]) -> str:
        """Extract tool name from OpenSearch source"""
        # 1. Legacy tool_name field
        tool_name = source.get('tool_name', '')
        if tool_name and tool_name != 'unknown':
            return tool_name
        
        # 2. Check active tools in tools structure
        tools = source.get('tools', {})
        active_tools = []
        
        if tools.get('bda_indexer') and len(tools['bda_indexer']) > 0:
            active_tools.append('bda_indexer')
        if tools.get('pdf_text_extractor') and len(tools['pdf_text_extractor']) > 0:
            active_tools.append('pdf_text_extractor')
        if tools.get('image_analysis') and len(tools['image_analysis']) > 0:
            active_tools.append('image_analysis')
        
        if active_tools:
            return '+'.join(active_tools)
        
        # 3. If content_combined exists, return combined_analysis
        if source.get('content_combined'):
            return 'combined_analysis'
        
        return 'unknown'
    
    def _create_initial_state(self, user_query: str, previous_analysis_context: str) -> AgentState:
        """Create initial AgentState"""
        logger.info("ğŸ”§ Creating initial state...")
        
        # Get max_iterations from environment variable
        max_iterations = int(os.environ.get('MAX_ITERATIONS', '10'))
        logger.info(f"ğŸ”¢ Max iterations set to: {max_iterations}")
        
        initial_state: AgentState = {
            "messages": [HumanMessage(content=user_query)],
            "index_id": self.index_id,
            "document_id": self.document_id,
            "segment_id": self.segment_id,
            "segment_index": self.segment_index,
            "file_path": self.file_path,
            "file_uri": self.file_path,  # VideoAnalyzerToolìš© ë³„ì¹­
            "image_path": self.image_uri,
            "session_id": f"session_{int(time.time())}",
            "thread_id": f"thread_{int(time.time())}",
            "user_query": user_query,
            "previous_analysis_context": previous_analysis_context,
            "current_step": 0,
            "max_iterations": max_iterations,
            "tools_used": [],
            "tool_results": [],
            "tool_references": [],  # ìƒˆë¡œ ì¶”ê°€ëœ í•„ë“œ
            "tool_content": "",  # ìƒˆë¡œ ì¶”ê°€ëœ í•„ë“œ
            "analysis_history": [],
            "combined_analysis_context": previous_analysis_context,  # initial value
            "skip_opensearch_query": True,  # already queried, skip
            "enable_opensearch": bool(self.opensearch_service),
            "segment_type": self.segment_type,
            "start_timecode_smpte": self.start_timecode_smpte,
            "end_timecode_smpte": self.end_timecode_smpte,
            "media_type": self.media_type
        }
        
        logger.info("âœ… Initial state created")
        return initial_state
    
    def _create_config(self) -> RunnableConfig:
        """Create execution configuration"""
        # Get max_iterations from environment variable
        max_iterations = int(os.environ.get('MAX_ITERATIONS', '10'))
        
        # Use the stored thread_id for conversation continuity
        logger.info(f"ğŸ”„ Using stored thread_id: {self.thread_id}")
        
        return RunnableConfig(
            configurable={
                "thread_id": self.thread_id,
                "max_iterations": max_iterations
            }
        )
    
    def _execute_graph(self, initial_state: AgentState, config: RunnableConfig) -> AgentState:
        """Execute graph"""
        logger.info("âš¡ LangGraph execution started...")
        
        try:
            # Monitor progress with streaming execution
            final_state = None
            step_count = 0
            
            for chunk in self.graph.stream(initial_state, config):
                step_count += 1
                
                if isinstance(chunk, tuple) and len(chunk) == 2:
                    node_name, node_output = chunk
                    logger.info(f"ğŸ”„ Step {step_count}: {node_name} executed")
                    
                    if isinstance(node_output, dict):
                        final_state = node_output
                elif isinstance(chunk, dict):
                    final_state = chunk
            
            if final_state is None:
                logger.warning("âš ï¸ Graph execution completed but final_state is None")
                # Try fallback with invoke
                final_state = self.graph.invoke(initial_state, config)
            
            logger.info(f"âœ… LangGraph execution completed - total {step_count} steps")
            return final_state
            
        except Exception as e:
            logger.error(f"âŒ Graph execution error: {str(e)}")
            import traceback
            logger.error(f"âŒ Graph execution traceback: {traceback.format_exc()}")
            
            # Try fallback with invoke
            try:
                logger.info("ğŸ”„ Fallback: invoke mode retry")
                final_state = self.graph.invoke(initial_state, config)
                logger.info("âœ… Fallback invoke ì„±ê³µ")
                return final_state
            except Exception as fallback_error:
                logger.error(f"âŒ Fallback invokeë„ ì‹¤íŒ¨: {str(fallback_error)}")
                logger.error(f"âŒ Fallback traceback: {traceback.format_exc()}")
                raise e  # ì›ë˜ ì—ëŸ¬ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œí‚´
    
    def _process_results(self, final_state: AgentState, start_time: float) -> Dict[str, Any]:
        """Process and return results"""
        logger.info("ğŸ”„ _process_results í•¨ìˆ˜ í˜¸ì¶œë¨")
        execution_time = time.time() - start_time
        
        # Extract information from state
        tools_used = final_state.get('tools_used', [])
        tool_results = final_state.get('tool_results', [])
        analysis_history = final_state.get('analysis_history', [])
        steps_count = final_state.get('current_step', 0)
        
        # ë””ë²„ê¹…: final_state êµ¬ì¡° ë¶„ì„
        logger.info(f"ğŸ” final_state êµ¬ì¡° ë¶„ì„:")
        logger.info(f"   - final_state í‚¤ë“¤: {list(final_state.keys()) if isinstance(final_state, dict) else 'dictê°€ ì•„ë‹˜'}")
        logger.info(f"   - tools_used: {len(tools_used)} í•­ëª©")
        logger.info(f"   - tool_results: {len(tool_results)} í•­ëª©")
        logger.info(f"   - analysis_history: {len(analysis_history)} í•­ëª©")
        logger.info(f"   - current_step: {steps_count}")
        
        # íŠ¹ë³„í•œ í‚¤ë“¤ í™•ì¸
        special_keys = ['analysis_content', 'final_content', 'result', 'output', 'response']
        for key in special_keys:
            if key in final_state:
                value = final_state[key]
                if isinstance(value, str) and len(value.strip()) > 10:
                    logger.info(f"   - ë°œê²¬ëœ íŠ¹ë³„ í‚¤ '{key}': {len(value)}ì")
                else:
                    logger.info(f"   - íŠ¹ë³„ í‚¤ '{key}': {type(value)} (ë‚´ìš© ì—†ìŒ)")
        
        # Extract final AI message - final_state êµ¬ì¡° í™•ì¸ ë° ë©”ì‹œì§€ ì¶”ì¶œ
        messages = []
        
        # final_stateê°€ ì¤‘ì²© êµ¬ì¡°ì¸ì§€ í™•ì¸
        if 'model' in final_state and isinstance(final_state['model'], dict) and 'messages' in final_state['model']:
            messages = final_state['model']['messages']
            logger.info("ğŸ” ì¤‘ì²©ëœ model.messagesì—ì„œ ë©”ì‹œì§€ ì¶”ì¶œ")
        else:
            messages = final_state.get('messages', [])
            logger.info("ğŸ” ì§ì ‘ messagesì—ì„œ ë©”ì‹œì§€ ì¶”ì¶œ")
        
        final_content = ""
        
        logger.info(f"ğŸ” ë©”ì‹œì§€ ì¶”ì¶œ ë””ë²„ê¹…:")
        logger.info(f"   - ì „ì²´ ë©”ì‹œì§€ ìˆ˜: {len(messages)}")
        logger.info(f"   - final_state ìµœìƒìœ„ í‚¤ë“¤: {list(final_state.keys())}")
        if 'model' in final_state:
            model_keys = list(final_state['model'].keys()) if isinstance(final_state['model'], dict) else 'dictê°€ ì•„ë‹˜'
            logger.info(f"   - final_state.model í‚¤ë“¤: {model_keys}")
        
        # ë©”ì‹œì§€ íƒ€ì…ë³„ ë¶„ì„
        message_types = {}
        for i, msg in enumerate(messages):
            msg_type = type(msg).__name__
            message_types[msg_type] = message_types.get(msg_type, 0) + 1
            logger.info(f"   - ë©”ì‹œì§€ {i}: {msg_type}")
            
            # ë©”ì‹œì§€ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 100ì)
            if hasattr(msg, 'content') and msg.content:
                content_preview = str(msg.content)[:100] + "..." if len(str(msg.content)) > 100 else str(msg.content)
                logger.info(f"     ë‚´ìš©: {content_preview}")
        
        logger.info(f"   - ë©”ì‹œì§€ íƒ€ì…ë³„ í†µê³„: {message_types}")
        
        from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
        
        # AI ë©”ì‹œì§€ ì¶”ì¶œ (ê°œì„ ëœ ë°©ë²•ë“¤)
        ai_messages = []
        
        # ë°©ë²• 1: AIMessage íƒ€ì… ì²´í¬
        for msg in messages:
            if isinstance(msg, AIMessage):
                ai_messages.append(msg)
        
        logger.info(f"   - AIMessage íƒ€ì… ë©”ì‹œì§€ ìˆ˜: {len(ai_messages)}")
        
        # ë°©ë²• 2: íƒ€ì… ì´ë¦„ìœ¼ë¡œ ì²´í¬ (fallback)
        if not ai_messages:
            for msg in messages:
                if type(msg).__name__ == 'AIMessage':
                    ai_messages.append(msg)
            logger.info(f"   - íƒ€ì… ì´ë¦„ ê¸°ë°˜ AIMessage ìˆ˜: {len(ai_messages)}")
        
        # ë°©ë²• 3: hasattrë¡œ AIMessage ì†ì„± í™•ì¸
        if not ai_messages:
            for msg in messages:
                if hasattr(msg, 'content') and hasattr(msg, 'response_metadata'):
                    ai_messages.append(msg)
            logger.info(f"   - ì†ì„± ê¸°ë°˜ AIMessage ìˆ˜: {len(ai_messages)}")
        
        # ë°©ë²• 4: contentê°€ ìˆê³  ì¶©ë¶„íˆ ê¸´ ë©”ì‹œì§€ (ìµœí›„ ìˆ˜ë‹¨)
        if not ai_messages:
            content_messages = [msg for msg in messages if hasattr(msg, 'content') and msg.content and len(str(msg.content).strip()) > 50]
            if content_messages:
                # ê°€ì¥ ê¸´ ë‚´ìš©ì„ ê°€ì§„ ë©”ì‹œì§€ ì„ íƒ
                longest_msg = max(content_messages, key=lambda m: len(str(m.content)) if m.content else 0)
                ai_messages = [longest_msg]
                logger.info(f"   - ê°€ì¥ ê¸´ ì½˜í…ì¸  ë©”ì‹œì§€ ì¶”ì¶œ: {len(str(longest_msg.content))}ì")
        
        if ai_messages:
            last_ai_msg = ai_messages[-1]
            if hasattr(last_ai_msg, 'content') and last_ai_msg.content:
                final_content = str(last_ai_msg.content)
                logger.info(f"   - ì¶”ì¶œëœ ìµœì¢… ì‘ë‹µ ê¸¸ì´: {len(final_content)}")
                logger.info(f"   - ìµœì¢… ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {final_content[:200]}...")
            else:
                logger.warning(f"   - AI ë©”ì‹œì§€ì— contentê°€ ì—†ìŒ: {type(last_ai_msg)}")
        else:
            logger.warning("   - AI ë©”ì‹œì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            
            # ë””ë²„ê¹…ì„ ìœ„í•´ ëª¨ë“  ë©”ì‹œì§€ ë‚´ìš© ì¶œë ¥
            logger.warning("   - ì „ì²´ ë©”ì‹œì§€ ë‚´ìš© ë¤í”„:")
            for i, msg in enumerate(messages):
                logger.warning(f"     ë©”ì‹œì§€ {i}: {type(msg).__name__}")
                if hasattr(msg, 'content'):
                    content_str = str(msg.content)[:300] + "..." if len(str(msg.content)) > 300 else str(msg.content)
                    logger.warning(f"       ë‚´ìš©: {content_str}")
                else:
                    logger.warning(f"       ë‚´ìš©: content ì†ì„± ì—†ìŒ")
        
        # ì¶”ê°€: final_stateì—ì„œ ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ì½˜í…ì¸  ì°¾ê¸°
        if not final_content:
            logger.info("ğŸ” ëŒ€ì•ˆì  ì½˜í…ì¸  ì¶”ì¶œ ì‹œë„:")
            final_content = self._extract_content_from_state(final_state)
        
        # ì—¬ì „íˆ ì½˜í…ì¸ ê°€ ì—†ìœ¼ë©´ ì¢…í•© ë¶„ì„ ìƒì„±
        if not final_content or len(final_content.strip()) < 100:
            logger.info("ğŸ” ì½˜í…ì¸  ë¶€ì¡±ìœ¼ë¡œ ì¢…í•© ë¶„ì„ ìƒì„±")
            final_content = self._generate_fallback_analysis(final_state)
        
        # ìµœì¢… í™•ì¸
        if final_content:
            logger.info(f"âœ… ìµœì¢… ì½˜í…ì¸  ì¶”ì¶œ ì„±ê³µ! ê¸¸ì´: {len(final_content)}ì")
        else:
            logger.error("âŒ ìµœì¢… ì½˜í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨ - ëª¨ë“  ë°©ë²• ì‹¤íŒ¨")
            logger.error("âŒ final_state ì „ì²´ ë¤í”„:")
            import json
            try:
                state_dump = json.dumps(final_state, indent=2, default=str, ensure_ascii=False)[:2000]
                logger.error(f"   State dump (ì²˜ìŒ 2000ì): {state_dump}")
            except Exception as dump_error:
                logger.error(f"   State dump ì‹¤íŒ¨: {dump_error}")
                logger.error(f"   State type: {type(final_state)}")
                logger.error(f"   State keys: {list(final_state.keys()) if hasattr(final_state, 'keys') else 'keys() ì—†ìŒ'}")
        
        # Create analysis summary
        analysis_summary = final_content[:1000] + "..." if len(final_content) > 1000 else final_content
        
        # Log results
        logger.info(f"ğŸ“Š Analysis result summary:")
        logger.info(f"  - Execution time: {execution_time:.2f} seconds")
        logger.info(f"  - Execution steps: {steps_count}")
        logger.info(f"  - Used tools: {tools_used}")
        logger.info(f"  - Tool results: {len(tool_results)}")
        logger.info(f"  - Analysis history: {len(analysis_history)}")
        logger.info(f"  - Final content: {len(final_content)} characters")

        # Aggregate and log token usage from tools (if provided by tools)
        try:
            token_input_sum = 0
            token_output_sum = 0
            token_total_sum = 0
            per_tool_tokens = []
            for tr in tool_results:
                if isinstance(tr, dict):
                    wrapper = tr.get('data', {})
                    inner = wrapper.get('data', {}) if isinstance(wrapper, dict) else {}
                    tok = inner.get('token_usage') if isinstance(inner, dict) else None
                    if isinstance(tok, dict):
                        ti = tok.get('input_tokens') or 0
                        to = tok.get('output_tokens') or 0
                        tt = tok.get('total_tokens') or 0
                        # Safe int conversion
                        ti = int(ti) if isinstance(ti, (int, float)) or (isinstance(ti, str) and ti.isdigit()) else 0
                        to = int(to) if isinstance(to, (int, float)) or (isinstance(to, str) and to.isdigit()) else 0
                        tt = int(tt) if isinstance(tt, (int, float)) or (isinstance(tt, str) and tt.isdigit()) else (ti + to)
                        token_input_sum += ti
                        token_output_sum += to
                        token_total_sum += tt
                        per_tool_tokens.append((tr.get('tool_name', 'unknown'), ti, to, tt))
            logger.info(f"ğŸ”¢ Token usage by tool: {per_tool_tokens}")
            if per_tool_tokens:
                for name, ti, to, tt in per_tool_tokens:
                    logger.info(f"ğŸ”¢ Token usage by tool [{name}]: input={ti}, output={to}, total={tt}")
                logger.info(f"ğŸ”¢ Token usage total (tools): input={token_input_sum}, output={token_output_sum}, total={token_total_sum}")
        except Exception as token_err:
            logger.debug(f"Token usage aggregation skipped: {str(token_err)}")
        
        # Preview final analysis content (for development)
        if final_content:
            preview = final_content[:500] + "..." if len(final_content) > 500 else final_content
            logger.info(f"ğŸ“ Final analysis content preview:\n{preview}")
        
        # Save final AI response to OpenSearch
        opensearch_saved = self._save_final_ai_response_to_opensearch(final_content, final_state)
        logger.info(f"ğŸ’¾ OpenSearch ì €ì¥ ìµœì¢… ê²°ê³¼: {opensearch_saved}")
        
        # Save summary to Segments table
        segments_summary_saved = self._save_summary_to_segments_table(final_content)
        logger.info(f"ğŸ’¾ Segments í…Œì´ë¸” summary ì €ì¥ ìµœì¢… ê²°ê³¼: {segments_summary_saved}")
        
        # Extract references from final_state for streaming
        final_references = final_state.get('tool_references', [])
        logger.info(f"ğŸ“‹ ìµœì¢… ì°¸ì¡° ê°œìˆ˜: {len(final_references)}")
        
        return {
            'success': True,
            'document_id': self.document_id,
            'segment_id': self.segment_id,
            'segment_index': self.segment_index,
            'analysis_content': final_content,
            'analysis_summary': analysis_summary,
            'analysis_time': execution_time,
            'steps_count': steps_count,
            'tools_used': tools_used,
            'tool_results': tool_results,
            'analysis_history': analysis_history,
            'references': final_references,  # ì°¸ì¡° ì •ë³´ ì¶”ê°€
            'opensearch_saved': opensearch_saved,  # ì‹¤ì œ ì €ì¥ ê²°ê³¼ í¬í•¨
            'segments_summary_saved': segments_summary_saved,  # Segments í…Œì´ë¸” summary ì €ì¥ ê²°ê³¼
            'timestamp': datetime.now(timezone.utc).isoformat()
        }
    
    def _save_final_ai_response_to_opensearch(self, final_content: str, final_state: AgentState):
        """ìµœì¢… AI ì‘ë‹µì„ OpenSearchì— ì €ì¥"""
        logger.info("ğŸ”„ _save_final_ai_response_to_opensearch í•¨ìˆ˜ í˜¸ì¶œë¨")
        try:
            logger.info(f"ğŸ“ ìµœì¢… ì‘ë‹µ ê¸¸ì´: {len(final_content) if final_content else 0}")
            
            if not final_content or len(final_content.strip()) < 10:
                logger.info("ğŸ“ ì‘ë‹µ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ì•„ ìµœì¢… ì‘ë‹µ ì €ì¥ ê±´ë„ˆëœ€")
                return False
            
            logger.info(f"ğŸ” OpenSearch ì„œë¹„ìŠ¤ ìƒíƒœ: {self.opensearch_service is not None}")
            if not self.opensearch_service:
                logger.warning("âš ï¸ OpenSearch ì„œë¹„ìŠ¤ê°€ ì—†ì–´ ìµœì¢… ì‘ë‹µ ì €ì¥ ê±´ë„ˆëœ€")
                return False
            
            # ì‚¬ìš©ì ì¿¼ë¦¬ ì¶”ì¶œ
            user_query = (f"ë¬¸ì„œ '{self.document_id}' "
                         f"í˜ì´ì§€ {self.segment_index + 1}ë¥¼ ë‹¤ì–‘í•œ ê°ë„ì—ì„œ ë„êµ¬ë¥¼ í™œìš©í•˜ì—¬ "
                         f"ë¶„ì„í•˜ê³  ìƒì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.")
            logger.info(f"ğŸ’¬ ì‚¬ìš©ì ì¿¼ë¦¬: {user_query[:100]}...")
            
            # OpenSearch ì €ì¥ ì‹œë„ ì „ ìƒì„¸ ì •ë³´ ë¡œê·¸
            logger.info("ğŸ’¾ OpenSearchì— ìµœì¢… AI ì‘ë‹µ ì €ì¥ ì‹œë„ ì¤‘...")
            logger.info(f"   - segment_id: {self.segment_id}")
            logger.info(f"   - document_id: {self.document_id}")
            logger.info(f"   - segment_index: {self.segment_index}")
            logger.info(f"   - analysis_query ê¸¸ì´: {len(user_query)}")
            logger.info(f"   - content ê¸¸ì´: {len(final_content)}")
            logger.info(f"   - analysis_steps: final_ai_response")
            
            # segment-unit ë°©ì‹ìœ¼ë¡œ ìµœì¢… AI ì‘ë‹µì„ ai_analysis ë„êµ¬ë¡œ ì €ì¥
            success = self.opensearch_service.add_ai_analysis_tool(
                index_id=self.index_id,
                document_id=self.document_id,
                segment_id=self.segment_id,
                segment_index=self.segment_index,
                analysis_query=user_query,
                content=final_content,
                analysis_steps="final_ai_response",
                analysis_type="final_ai_response",
                media_type=self.media_type
            )
            logger.info(f"ğŸ’¾ OpenSearch ì €ì¥ ê²°ê³¼: {success}")
            
            if success:
                logger.info(f"âœ… OpenSearch ìµœì¢… AI ì‘ë‹µ ì €ì¥ ì™„ë£Œ")
                logger.info(f"ğŸ’¾ ì €ì¥ëœ ë°ì´í„°: segment_id={self.segment_id}, query={user_query[:50]}...")
                
                # ì„ë² ë”© ì—…ë°ì´íŠ¸ë„ ì‹œë„
                try:
                    logger.info("ğŸ”„ ì„ë² ë”© ì—…ë°ì´íŠ¸ ì‹œë„ ì¤‘...")
                    embedding_success = self.opensearch_service.update_segment_embeddings(self.index_id, self.segment_id)
                    logger.info(f"ğŸ”„ ì„ë² ë”© ì—…ë°ì´íŠ¸ ê²°ê³¼: {embedding_success}")
                except Exception as embedding_error:
                    logger.warning(f"âš ï¸ ì„ë² ë”© ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {str(embedding_error)}")
                
                return True
            else:
                logger.error(f"âŒ OpenSearch ìµœì¢… AI ì‘ë‹µ ì €ì¥ ì‹¤íŒ¨ (success=False)")
                return False
            
        except Exception as e:
            logger.error(f"âŒ OpenSearch ìµœì¢… AI ì‘ë‹µ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            logger.error(f"âŒ ì˜ˆì™¸ íƒ€ì…: {type(e).__name__}")
            import traceback
            logger.error(f"âŒ ì˜¤ë¥˜ ìƒì„¸ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:")
            logger.error(traceback.format_exc())
            
            # ì¶”ê°€ ë””ë²„ê¹… ì •ë³´
            logger.error("ğŸ” ì¶”ê°€ ë””ë²„ê¹… ì •ë³´:")
            logger.error(f"   - OpenSearch ì„œë¹„ìŠ¤ ê°ì²´: {type(self.opensearch_service) if self.opensearch_service else None}")
            logger.error(f"   - OpenSearch ì—”ë“œí¬ì¸íŠ¸: {getattr(self.opensearch_service, 'endpoint', 'N/A') if self.opensearch_service else 'N/A'}")
            logger.error(f"   - OpenSearch ì¸ë±ìŠ¤: {getattr(self.opensearch_service, 'index_name', 'N/A') if self.opensearch_service else 'N/A'}")
            logger.error(f"   - ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´: segment_id={self.segment_id}")
            logger.error(f"   - ì½˜í…ì¸  ì •ë³´: ê¸¸ì´={len(final_content) if final_content else 0}")
            
            # ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œí‚¤ì§€ ì•Šê³  False ë°˜í™˜ìœ¼ë¡œ ì‹¤íŒ¨ë¥¼ ëª…í™•íˆ í‘œì‹œ
            return False
    
    def _save_summary_to_segments_table(self, final_content: str) -> bool:
        """Segments í…Œì´ë¸”ì— summaryë¥¼ ì €ì¥"""
        logger.info("ğŸ”„ _save_summary_to_segments_table í•¨ìˆ˜ í˜¸ì¶œë¨")
        
        if not final_content or len(final_content.strip()) < 10:
            logger.info("ğŸ“ ìš”ì•½ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ì•„ ì €ì¥ ê±´ë„ˆëœ€")
            return False
        
        if not self.dynamodb_service:
            logger.warning("âš ï¸ DynamoDB ì„œë¹„ìŠ¤ê°€ ì—†ì–´ summary ì €ì¥ ê±´ë„ˆëœ€")
            return False
        
        if not self.segment_id:
            logger.warning("âš ï¸ segment_idê°€ ì—†ì–´ summary ì €ì¥ ê±´ë„ˆëœ€")
            return False
        
        try:
            # Segments í…Œì´ë¸”ì— summary ì—…ë°ì´íŠ¸
            logger.info(f"ğŸ’¾ Segments í…Œì´ë¸”ì— summary ì €ì¥ ì‹œë„ ì¤‘...")
            logger.info(f"   - segment_id: {self.segment_id}")
            logger.info(f"   - summary ê¸¸ì´: {len(final_content)}")
            
            # Summary ë‚´ìš© ê¸¸ì´ ì œí•œ (DynamoDB ì•„ì´í…œ í¬ê¸° ì œí•œ ê³ ë ¤)
            max_summary_length = 30000  # 30KB ì œí•œ
            if len(final_content) > max_summary_length:
                summary_content = final_content[:max_summary_length] + "...[ìš”ì•½ë¨]"
                logger.info(f"   - summary ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ì–´ {max_summary_length}ìë¡œ ì œí•œë¨")
            else:
                summary_content = final_content
            
            # DynamoDB ì—…ë°ì´íŠ¸ ë°ì´í„° êµ¬ì„±
            update_data = {
                'summary': summary_content,
                'analysis_completed_at': datetime.now(timezone.utc).isoformat(),
                'updated_at': datetime.now(timezone.utc).isoformat()
            }
            
            # Segments í…Œì´ë¸” ì—…ë°ì´íŠ¸
            success = self.dynamodb_service.update_item(
                table_name='segments',
                key={'segment_id': self.segment_id},
                updates=update_data
            )
            
            if success:
                logger.info(f"âœ… Segments í…Œì´ë¸”ì— summary ì €ì¥ ì™„ë£Œ")
                logger.info(f"ğŸ’¾ ì €ì¥ëœ ë°ì´í„°: segment_id={self.segment_id}, summary_length={len(summary_content)}")
                return True
            else:
                logger.error(f"âŒ Segments í…Œì´ë¸” summary ì €ì¥ ì‹¤íŒ¨ (success=False)")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Segments í…Œì´ë¸” summary ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            logger.error(f"âŒ ì˜ˆì™¸ íƒ€ì…: {type(e).__name__}")
            import traceback
            logger.error(f"âŒ ì˜¤ë¥˜ ìƒì„¸ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:")
            logger.error(traceback.format_exc())
            return False
    
    def _extract_content_from_state(self, final_state: AgentState) -> str:
        """final_stateì—ì„œ ì½˜í…ì¸  ì¶”ì¶œ (ì—¬ëŸ¬ ì „ëµ ì‚¬ìš©)"""
        logger.info("ğŸ” _extract_content_from_state ì‹œì‘")
        
        # analysis_historyì—ì„œ ì¶”ì¶œ ì‹œë„
        analysis_history = final_state.get('analysis_history', [])
        if analysis_history:
            logger.info(f"   - analysis_history í•­ëª© ìˆ˜: {len(analysis_history)}")
            for item in reversed(analysis_history):  # ìµœê·¼ ê²ƒë¶€í„°
                if isinstance(item, dict):
                    for key in ['content', 'result', 'output', 'analysis']:
                        if key in item:
                            content_candidate = str(item[key])
                            if len(content_candidate.strip()) > 50:
                                logger.info(f"   - analysis_history['{key}']ì—ì„œ ì½˜í…ì¸  ì¶”ì¶œ: {len(content_candidate)}ì")
                                return content_candidate
        
        # tool_resultsì—ì„œ ì¶”ì¶œ ì‹œë„
        tool_results = final_state.get('tool_results', [])
        if tool_results:
            logger.info(f"   - tool_results í•­ëª© ìˆ˜: {len(tool_results)}")
            for result in reversed(tool_results):  # ìµœê·¼ ê²ƒë¶€í„°
                if isinstance(result, dict):
                    for key in ['result', 'content', 'output', 'analysis']:
                        if key in result:
                            content_candidate = str(result[key])
                            if len(content_candidate.strip()) > 50:
                                logger.info(f"   - tool_results['{key}']ì—ì„œ ì½˜í…ì¸  ì¶”ì¶œ: {len(content_candidate)}ì")
                                return content_candidate
        
        # combined_analysis_contextì—ì„œ ì¶”ì¶œ
        combined_context = final_state.get('combined_analysis_context', '')
        if combined_context and len(combined_context.strip()) > 50:
            logger.info(f"   - combined_analysis_contextì—ì„œ ì½˜í…ì¸  ì¶”ì¶œ: {len(combined_context)}ì")
            return combined_context
        
        # final_stateì˜ ëª¨ë“  ê°’ì—ì„œ ê¸´ ë¬¸ìì—´ ì°¾ê¸° (ìµœí›„ ìˆ˜ë‹¨)
        logger.info("ğŸ” final_state ì „ì²´ì—ì„œ ì½˜í…ì¸  ì°¾ê¸°:")
        for key, value in final_state.items():
            if isinstance(value, str) and len(value.strip()) > 100:
                logger.info(f"   - '{key}' í‚¤ì—ì„œ ì½˜í…ì¸  ë°œê²¬: {len(value)}ì")
                return value
            elif isinstance(value, list) and value:
                # ë¦¬ìŠ¤íŠ¸ì˜ ë§ˆì§€ë§‰ í•­ëª©ì´ ë¬¸ìì—´ì¸ì§€ í™•ì¸
                last_item = value[-1]
                if isinstance(last_item, str) and len(last_item.strip()) > 100:
                    logger.info(f"   - '{key}' ë¦¬ìŠ¤íŠ¸ì˜ ë§ˆì§€ë§‰ í•­ëª©ì—ì„œ ì½˜í…ì¸  ë°œê²¬: {len(last_item)}ì")
                    return last_item
                elif isinstance(last_item, dict) and 'content' in last_item:
                    content_candidate = str(last_item['content'])
                    if len(content_candidate.strip()) > 100:
                        logger.info(f"   - '{key}' ë¦¬ìŠ¤íŠ¸ í•­ëª©ì˜ contentì—ì„œ ë°œê²¬: {len(content_candidate)}ì")
                        return content_candidate
        
        logger.info("   - ì¶”ì¶œ ê°€ëŠ¥í•œ ì½˜í…ì¸ ë¥¼ ì°¾ì§€ ëª»í•¨")
        return ""
    
    def _generate_fallback_analysis(self, final_state: AgentState) -> str:
        """ì½˜í…ì¸  ì¶”ì¶œì— ì‹¤íŒ¨í–ˆì„ ë•Œ fallback ë¶„ì„ ìƒì„±"""
        logger.info("ğŸ“ _generate_fallback_analysis ì‹œì‘")
        
        # ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘
        document_id = final_state.get('document_id', self.document_id)
        segment_id = final_state.get('segment_id', self.segment_id)
        user_query = final_state.get('user_query', '')
        current_step = final_state.get('current_step', 0)
        
        # Stateì—ì„œ ìœ ìš©í•œ ì •ë³´ ìˆ˜ì§‘
        analysis_history = final_state.get('analysis_history', [])
        tool_results = final_state.get('tool_results', [])
        tools_used = final_state.get('tools_used', [])
        combined_context = final_state.get('combined_analysis_context', '')
        
        # ì‹¤í–‰ëœ ë„êµ¬ ì •ë³´
        tools_info = []
        for entry in analysis_history:
            if isinstance(entry, dict):
                tool_name = entry.get('tool_name', 'Unknown')
                success = entry.get('success', False)
                result_preview = str(entry.get('result', ''))[:200] + "..." if entry.get('result') else "No result"
                tools_info.append({
                    'tool': tool_name,
                    'success': success,
                    'preview': result_preview
                })
        
        # Fallback ë¶„ì„ ìƒì„±
        fallback_analysis = f"""# ë¬¸ì„œ ë¶„ì„ ê²°ê³¼ (Fallback ìƒì„±)

## ğŸ“‹ ë¶„ì„ ê°œìš”
- **ë¬¸ì„œ ID**: {document_id}
- **ì„¸ê·¸ë¨¼íŠ¸ ID**: {segment_id}
- **ë¶„ì„ ìš”ì²­**: {user_query[:300]}{'...' if len(user_query) > 300 else ''}
- **ì‹¤í–‰ ë‹¨ê³„**: {current_step}
- **ìƒíƒœ**: ë¶„ì„ ì™„ë£Œ (Fallback ìƒì„±)

## ğŸ› ï¸ ì‹¤í–‰ëœ ë¶„ì„ ë„êµ¬
"""
        
        if tools_info:
            for i, tool_info in enumerate(tools_info, 1):
                status = "âœ…" if tool_info['success'] else "âŒ"
                fallback_analysis += f"""
### {i}. {tool_info['tool']} {status}
- **ì‹¤í–‰ ìƒíƒœ**: {"ì„±ê³µ" if tool_info['success'] else "ì‹¤íŒ¨"}
- **ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°**: {tool_info['preview']}
"""
        else:
            fallback_analysis += "\n- ì‹¤í–‰ëœ ë„êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
        
        # ì¢…í•© ë¶„ì„ ì»¨í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš° í¬í•¨
        if combined_context and len(combined_context.strip()) > 10:
            fallback_analysis += f"""

## ğŸ”— ìˆ˜ì§‘ëœ ë¶„ì„ ì •ë³´
{combined_context[:800]}{'...' if len(combined_context) > 800 else ''}
"""
        
        fallback_analysis += f"""

## âš ï¸ ë¶„ì„ ì™„ë£Œ ì •ë³´
ì´ ê²°ê³¼ëŠ” ì‹œìŠ¤í…œì—ì„œ ìë™ìœ¼ë¡œ ìƒì„±ëœ fallback ë¶„ì„ì…ë‹ˆë‹¤. 
ì›ë³¸ AI ëª¨ë¸ì˜ ì‘ë‹µì„ ì¶”ì¶œí•  ìˆ˜ ì—†ì–´ ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.

- **ì´ ì‹¤í–‰ ë‹¨ê³„**: {current_step}
- **ì‚¬ìš©ëœ ë„êµ¬ ìˆ˜**: {len(tools_info)}
- **ìˆ˜ì§‘ëœ ì •ë³´ëŸ‰**: {len(combined_context)} ë¬¸ì

---
*ë¶„ì„ ìƒì„± ì‹œê°„: {datetime.now(timezone.utc).isoformat()}*
*ìƒì„± ë°©ì‹: Fallback Analysis*
"""
        
        logger.info(f"ğŸ“ Fallback ë¶„ì„ ìƒì„± ì™„ë£Œ - ê¸¸ì´: {len(fallback_analysis)} ë¬¸ì")
        logger.info(f"ğŸ“ í¬í•¨ëœ ë„êµ¬ ì •ë³´: {len(tools_info)}ê°œ")
        
        return fallback_analysis
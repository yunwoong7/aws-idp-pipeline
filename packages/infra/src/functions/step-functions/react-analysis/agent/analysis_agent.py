"""
Refactored AnalysisAgent - Optimized for LangGraph structure
"""

import logging
import time
from datetime import datetime, timezone
from typing import Dict, Any
from langchain_core.messages import HumanMessage
from langchain_core.runnables import RunnableConfig

# Common module imports
import sys
import os
sys.path.append('/opt/python')
from common import OpenSearchService
from common.dynamodb_service import DynamoDBService

from agent.llm import get_llm
from agent.state.agent_state import AgentState
from agent.graph.builder import create_analysis_graph

logger = logging.getLogger(__name__)


class AnalysisAgent:
    """
    LangGraph-based analysis agent
    """
    
    def __init__(self, 
                 index_id: str = None,
                 document_id: str = None,
                 segment_id: str = None,
                 segment_index: int = None,
                 image_uri: str = None,
                 file_path: str = None,
                 model_id: str = None,
                 max_tokens: int = 8192,
                 thread_id: str = None,
                 segment_type: str = None,
                 start_timecode_smpte: str = None,
                 end_timecode_smpte: str = None,
                 media_type: str = None):
        
        logger.info("üöÄ AnalysisAgent initialization started")
        
        # Set basic information
        self.index_id = index_id
        self.document_id = document_id
        self.segment_id = segment_id
        self.segment_index = segment_index
        self.image_uri = image_uri
        self.file_path = file_path
        
        # ÎèôÏòÅÏÉÅ Ï±ïÌÑ∞ Ï†ïÎ≥¥ (VideoAnalyzerToolÏö©)
        self.segment_type = segment_type
        self.start_timecode_smpte = start_timecode_smpte
        self.end_timecode_smpte = end_timecode_smpte
        
        # Î¨∏ÏÑú ÌÉÄÏûÖ Ï†ïÎ≥¥
        self.media_type = media_type or 'DOCUMENT'
        
        # Set thread_id for conversation continuity
        self.thread_id = thread_id or f"document_{document_id}_default"
        logger.info(f"üîÑ Thread ID set to: {self.thread_id}")
        
        # Initialize LLM model
        self.model = get_llm(model_id=model_id, max_tokens=max_tokens)
        
        # Initialize OpenSearch service (for previous analysis lookup)
        self.opensearch_service = self._init_opensearch()
        
        # Initialize DynamoDB service (for updating segments table)
        self.dynamodb_service = self._init_dynamodb()
        
        # Create LangGraph
        self.graph = create_analysis_graph(self.model)
        
        logger.info(f"‚úÖ AnalysisAgent initialization completed")
        logger.info(f"üìÅ Document: {document_id}, Segment: {segment_index}")

    
    def _init_opensearch(self):
        """Initialize OpenSearch service"""
        opensearch_endpoint = os.environ.get('OPENSEARCH_ENDPOINT')
        if not opensearch_endpoint:
            logger.warning("‚ö†Ô∏è OpenSearch endpoint is not set")
            return None
        
        try:
            service = OpenSearchService(
                endpoint=opensearch_endpoint,
                index_name=os.environ.get('OPENSEARCH_INDEX_NAME', 'aws-idp-ai-analysis'),
                region=os.environ.get('OPENSEARCH_REGION') or os.environ.get('AWS_REGION', 'us-west-2')
            )
            logger.info("‚úÖ OpenSearch service initialization completed")
            return service
        except Exception as e:
            logger.warning(f"‚ùå OpenSearch initialization failed: {str(e)}")
            return None
    
    def _init_dynamodb(self):
        """Initialize DynamoDB service"""
        try:
            service = DynamoDBService(region=os.environ.get('AWS_REGION', 'us-west-2'))
            logger.info("‚úÖ DynamoDB service initialization completed")
            return service
        except Exception as e:
            logger.warning(f"‚ùå DynamoDB initialization failed: {str(e)}")
            return None
    
    def analyze_document(self, user_query: str = None, analysis_type: str = "comprehensive") -> Dict[str, Any]:
        """
        Execute document analysis
        
        Args:
            user_query: User query
            analysis_type: Analysis type
            
        Returns:
            Analysis result
        """
        logger.info("-" * 50)
        logger.info("üéØ Document analysis started")
        logger.info("-" * 50)
        logger.info(f"üí¨ Analysis query: {user_query}")
        logger.info(f"üìã Analysis type: {analysis_type}")
        
        start_time = time.time()
        
        try:
            # 1. Get previous analysis context
            previous_analysis_context = self._get_previous_analysis()
            
            # 2. Create initial state
            initial_state = self._create_initial_state(user_query, previous_analysis_context)
            
            # 3. Create execution configuration
            config = self._create_config()
            
            # 4. Execute graph
            logger.info("üöÄ _execute_graph Ìò∏Ï∂ú ÏãúÏûë")
            final_state = self._execute_graph(initial_state, config)
            logger.info("‚úÖ _execute_graph ÏôÑÎ£å")
            
            # 5. Process results
            logger.info("üöÄ _process_results Ìò∏Ï∂ú ÏãúÏûë")
            result = self._process_results(final_state, start_time)
            logger.info("‚úÖ _process_results ÏôÑÎ£å")
            
            logger.info("-" * 50)
            logger.info("üéâ Document analysis completed")
            logger.info("-" * 50)
            
            return result
            
        except Exception as e:
            error_msg = f"Document analysis execution failed: {str(e)}"
            logger.error(f"‚ùå {error_msg}")
            import traceback
            logger.error(f"‚ùå ÏòàÏô∏ ÏÉÅÏÑ∏: {traceback.format_exc()}")
            
            return {
                'success': False,
                'error': str(e),
                'document_id': self.document_id,
                'timestamp': datetime.now(timezone.utc).isoformat()
            }
    
    def _get_previous_analysis(self) -> str:
        """Get previous analysis context"""
        if not self.opensearch_service:
            logger.info("üìã OpenSearch disabled - no previous analysis")
            return "**Previous analysis context**: OpenSearch disabled"
        
        try:
            logger.info("1. Get previous analysis context")
            
            filters = {
                "document_id": self.document_id,
                "segment_id": self.segment_id
            }
            
            response = self.opensearch_service.search_text(
                index_id=self.index_id,
                query="*",
                size=100,
                filters=filters
            )
            
            hits = response.get('hits', {}).get('hits', [])
            
            if not hits:
                logger.info("üìã No previous analysis results")
                return "**Previous analysis context**: No previous analysis results"
            
            # Construct analysis content
            context_parts = [f"**Previous analysis context** ({len(hits)} results):"]
            
            for i, hit in enumerate(hits, 1):
                source = hit.get('_source', {})
                
                # Extract actual content from new segment-unit structure
                content = self._extract_content_from_source(source)
                tool_name = self._extract_tool_name_from_source(source)
                created_at = source.get('created_at', '')
                
                if content and content.strip():
                    # If content is too long, summarize (use environment variable)
                    max_chars = int(os.environ.get('PREVIOUS_ANALYSIS_MAX_CHARACTERS', '100000'))
                    if len(content) > max_chars:
                        content_preview = content[:max_chars] + "...[summarized]"
                    else:
                        content_preview = content
                    
                    context_parts.append(f"\n{i}. **{tool_name}** ({created_at})")
                    context_parts.append(f"   {content_preview}")
            
            result = "\n".join(context_parts)
            logger.info(f"‚úÖ Previous analysis context lookup completed: {len(result)} characters")
            
            # Previous analysis context preview
            logger.info(f"üìã Previous analysis context preview:\n{result}")
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Previous analysis context lookup failed: {str(e)}")
            return f"**Previous analysis context**: Lookup failed ({str(e)})"
    
    def _extract_content_from_source(self, source: Dict[str, Any]) -> str:
        """Extract actual analysis content from OpenSearch source"""
        content_parts = []
        
        # 1. content_combined field (highest priority)
        content_combined = source.get('content_combined', '')
        if content_combined and content_combined.strip():
            content_parts.append(content_combined)
        
        # 2. Extract content from tools structure
        tools = source.get('tools', {})
        
        # bda_indexer tool content
        bda_tools = tools.get('bda_indexer', [])
        for bda_tool in bda_tools:
            if isinstance(bda_tool, dict):
                bda_content = bda_tool.get('content', '')
                if bda_content and bda_content.strip():
                    content_parts.append(f"[BDA Î∂ÑÏÑù] {bda_content}")
        
        # pdf_text_extractor tool content
        pdf_tools = tools.get('pdf_text_extractor', [])
        for pdf_tool in pdf_tools:
            if isinstance(pdf_tool, dict):
                pdf_content = pdf_tool.get('content', '')
                if pdf_content and pdf_content.strip():
                    content_parts.append(f"[PDF ÌÖçÏä§Ìä∏] {pdf_content}")
        
        # image_analysis tool content
        img_tools = tools.get('image_analysis', [])
        for img_tool in img_tools:
            if isinstance(img_tool, dict):
                img_content = img_tool.get('content', '')
                if img_content and img_content.strip():
                    content_parts.append(f"[Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù] {img_content}")
        
        # 3. Legacy content field (fallback)
        legacy_content = source.get('content', '')
        if legacy_content and legacy_content.strip() and not content_parts:
            content_parts.append(legacy_content)
        
        return '\n\n'.join(content_parts) if content_parts else ''
    
    def _extract_tool_name_from_source(self, source: Dict[str, Any]) -> str:
        """Extract tool name from OpenSearch source"""
        # 1. Legacy tool_name field
        tool_name = source.get('tool_name', '')
        if tool_name and tool_name != 'unknown':
            return tool_name
        
        # 2. Check active tools in tools structure
        tools = source.get('tools', {})
        active_tools = []
        
        if tools.get('bda_indexer') and len(tools['bda_indexer']) > 0:
            active_tools.append('bda_indexer')
        if tools.get('pdf_text_extractor') and len(tools['pdf_text_extractor']) > 0:
            active_tools.append('pdf_text_extractor')
        if tools.get('image_analysis') and len(tools['image_analysis']) > 0:
            active_tools.append('image_analysis')
        
        if active_tools:
            return '+'.join(active_tools)
        
        # 3. If content_combined exists, return combined_analysis
        if source.get('content_combined'):
            return 'combined_analysis'
        
        return 'unknown'
    
    def _create_initial_state(self, user_query: str, previous_analysis_context: str) -> AgentState:
        """Create initial AgentState"""
        logger.info("üîß Creating initial state...")
        
        # Get max_iterations from environment variable
        max_iterations = int(os.environ.get('MAX_ITERATIONS', '10'))
        logger.info(f"üî¢ Max iterations set to: {max_iterations}")
        
        initial_state: AgentState = {
            "messages": [HumanMessage(content=user_query)],
            "index_id": self.index_id,
            "document_id": self.document_id,
            "segment_id": self.segment_id,
            "segment_index": self.segment_index,
            "file_path": self.file_path,
            "file_uri": self.file_path,  # VideoAnalyzerToolÏö© Î≥ÑÏπ≠
            "image_path": self.image_uri,
            "session_id": f"session_{int(time.time())}",
            "thread_id": f"thread_{int(time.time())}",
            "user_query": user_query,
            "previous_analysis_context": previous_analysis_context,
            "current_step": 0,
            "max_iterations": max_iterations,
            "tools_used": [],
            "tool_results": [],
            "tool_references": [],  # ÏÉàÎ°ú Ï∂îÍ∞ÄÎêú ÌïÑÎìú
            "tool_content": "",  # ÏÉàÎ°ú Ï∂îÍ∞ÄÎêú ÌïÑÎìú
            "analysis_history": [],
            "combined_analysis_context": previous_analysis_context,  # initial value
            "skip_opensearch_query": True,  # already queried, skip
            "enable_opensearch": bool(self.opensearch_service),
            "segment_type": self.segment_type,
            "start_timecode_smpte": self.start_timecode_smpte,
            "end_timecode_smpte": self.end_timecode_smpte,
            "media_type": self.media_type
        }
        
        logger.info("‚úÖ Initial state created")
        return initial_state
    
    def _create_config(self) -> RunnableConfig:
        """Create execution configuration"""
        # Get max_iterations from environment variable
        max_iterations = int(os.environ.get('MAX_ITERATIONS', '10'))
        
        # Use the stored thread_id for conversation continuity
        logger.info(f"üîÑ Using stored thread_id: {self.thread_id}")
        
        return RunnableConfig(
            configurable={
                "thread_id": self.thread_id,
                "max_iterations": max_iterations
            }
        )
    
    def _execute_graph(self, initial_state: AgentState, config: RunnableConfig) -> AgentState:
        """Execute graph"""
        logger.info("‚ö° LangGraph execution started...")
        
        try:
            # Monitor progress with streaming execution
            final_state = None
            step_count = 0
            
            for chunk in self.graph.stream(initial_state, config):
                step_count += 1
                
                if isinstance(chunk, tuple) and len(chunk) == 2:
                    node_name, node_output = chunk
                    logger.info(f"üîÑ Step {step_count}: {node_name} executed")
                    
                    if isinstance(node_output, dict):
                        final_state = node_output
                elif isinstance(chunk, dict):
                    final_state = chunk
            
            if final_state is None:
                logger.warning("‚ö†Ô∏è Graph execution completed but final_state is None")
                # Try fallback with invoke
                final_state = self.graph.invoke(initial_state, config)
            
            logger.info(f"‚úÖ LangGraph execution completed - total {step_count} steps")
            return final_state
            
        except Exception as e:
            logger.error(f"‚ùå Graph execution error: {str(e)}")
            import traceback
            logger.error(f"‚ùå Graph execution traceback: {traceback.format_exc()}")
            
            # Try fallback with invoke
            try:
                logger.info("üîÑ Fallback: invoke mode retry")
                final_state = self.graph.invoke(initial_state, config)
                logger.info("‚úÖ Fallback invoke ÏÑ±Í≥µ")
                return final_state
            except Exception as fallback_error:
                logger.error(f"‚ùå Fallback invokeÎèÑ Ïã§Ìå®: {str(fallback_error)}")
                logger.error(f"‚ùå Fallback traceback: {traceback.format_exc()}")
                raise e  # ÏõêÎûò ÏóêÎü¨Î•º Îã§Ïãú Î∞úÏÉùÏãúÌÇ¥
    
    def _process_results(self, final_state: AgentState, start_time: float) -> Dict[str, Any]:
        """Process and return results"""
        logger.info("üîÑ _process_results Ìï®Ïàò Ìò∏Ï∂úÎê®")
        execution_time = time.time() - start_time
        
        # Extract information from state
        tools_used = final_state.get('tools_used', [])
        tool_results = final_state.get('tool_results', [])
        analysis_history = final_state.get('analysis_history', [])
        steps_count = final_state.get('current_step', 0)
        
        # ÎîîÎ≤ÑÍπÖ: final_state Íµ¨Ï°∞ Î∂ÑÏÑù
        logger.info(f"üîç final_state Íµ¨Ï°∞ Î∂ÑÏÑù:")
        logger.info(f"   - final_state ÌÇ§Îì§: {list(final_state.keys()) if isinstance(final_state, dict) else 'dictÍ∞Ä ÏïÑÎãò'}")
        logger.info(f"   - tools_used: {len(tools_used)} Ìï≠Î™©")
        logger.info(f"   - tool_results: {len(tool_results)} Ìï≠Î™©")
        logger.info(f"   - analysis_history: {len(analysis_history)} Ìï≠Î™©")
        logger.info(f"   - current_step: {steps_count}")
        
        # ÌäπÎ≥ÑÌïú ÌÇ§Îì§ ÌôïÏù∏
        special_keys = ['analysis_content', 'final_content', 'result', 'output', 'response']
        for key in special_keys:
            if key in final_state:
                value = final_state[key]
                if isinstance(value, str) and len(value.strip()) > 10:
                    logger.info(f"   - Î∞úÍ≤¨Îêú ÌäπÎ≥Ñ ÌÇ§ '{key}': {len(value)}Ïûê")
                else:
                    logger.info(f"   - ÌäπÎ≥Ñ ÌÇ§ '{key}': {type(value)} (ÎÇ¥Ïö© ÏóÜÏùå)")
        
        # Extract final AI message - final_state Íµ¨Ï°∞ ÌôïÏù∏ Î∞è Î©îÏãúÏßÄ Ï∂îÏ∂ú
        messages = []
        
        # final_stateÍ∞Ä Ï§ëÏ≤© Íµ¨Ï°∞Ïù∏ÏßÄ ÌôïÏù∏
        if 'model' in final_state and isinstance(final_state['model'], dict) and 'messages' in final_state['model']:
            messages = final_state['model']['messages']
            logger.info("üîç Ï§ëÏ≤©Îêú model.messagesÏóêÏÑú Î©îÏãúÏßÄ Ï∂îÏ∂ú")
        else:
            messages = final_state.get('messages', [])
            logger.info("üîç ÏßÅÏ†ë messagesÏóêÏÑú Î©îÏãúÏßÄ Ï∂îÏ∂ú")
        
        final_content = ""
        
        logger.info(f"üîç Î©îÏãúÏßÄ Ï∂îÏ∂ú ÎîîÎ≤ÑÍπÖ:")
        logger.info(f"   - Ï†ÑÏ≤¥ Î©îÏãúÏßÄ Ïàò: {len(messages)}")
        logger.info(f"   - final_state ÏµúÏÉÅÏúÑ ÌÇ§Îì§: {list(final_state.keys())}")
        if 'model' in final_state:
            model_keys = list(final_state['model'].keys()) if isinstance(final_state['model'], dict) else 'dictÍ∞Ä ÏïÑÎãò'
            logger.info(f"   - final_state.model ÌÇ§Îì§: {model_keys}")
        
        # Î©îÏãúÏßÄ ÌÉÄÏûÖÎ≥Ñ Î∂ÑÏÑù
        message_types = {}
        for i, msg in enumerate(messages):
            msg_type = type(msg).__name__
            message_types[msg_type] = message_types.get(msg_type, 0) + 1
            logger.info(f"   - Î©îÏãúÏßÄ {i}: {msg_type}")
            
            # Î©îÏãúÏßÄ ÎÇ¥Ïö© ÎØ∏Î¶¨Î≥¥Í∏∞ (Ï≤òÏùå 100Ïûê)
            if hasattr(msg, 'content') and msg.content:
                content_preview = str(msg.content)[:100] + "..." if len(str(msg.content)) > 100 else str(msg.content)
                logger.info(f"     ÎÇ¥Ïö©: {content_preview}")
        
        logger.info(f"   - Î©îÏãúÏßÄ ÌÉÄÏûÖÎ≥Ñ ÌÜµÍ≥Ñ: {message_types}")
        
        from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
        
        # AI Î©îÏãúÏßÄ Ï∂îÏ∂ú (Í∞úÏÑ†Îêú Î∞©Î≤ïÎì§)
        ai_messages = []
        
        # Î∞©Î≤ï 1: AIMessage ÌÉÄÏûÖ Ï≤¥ÌÅ¨
        for msg in messages:
            if isinstance(msg, AIMessage):
                ai_messages.append(msg)
        
        logger.info(f"   - AIMessage ÌÉÄÏûÖ Î©îÏãúÏßÄ Ïàò: {len(ai_messages)}")
        
        # Î∞©Î≤ï 2: ÌÉÄÏûÖ Ïù¥Î¶ÑÏúºÎ°ú Ï≤¥ÌÅ¨ (fallback)
        if not ai_messages:
            for msg in messages:
                if type(msg).__name__ == 'AIMessage':
                    ai_messages.append(msg)
            logger.info(f"   - ÌÉÄÏûÖ Ïù¥Î¶Ñ Í∏∞Î∞ò AIMessage Ïàò: {len(ai_messages)}")
        
        # Î∞©Î≤ï 3: hasattrÎ°ú AIMessage ÏÜçÏÑ± ÌôïÏù∏
        if not ai_messages:
            for msg in messages:
                if hasattr(msg, 'content') and hasattr(msg, 'response_metadata'):
                    ai_messages.append(msg)
            logger.info(f"   - ÏÜçÏÑ± Í∏∞Î∞ò AIMessage Ïàò: {len(ai_messages)}")
        
        # Î∞©Î≤ï 4: contentÍ∞Ä ÏûàÍ≥† Ï∂©Î∂ÑÌûà Í∏¥ Î©îÏãúÏßÄ (ÏµúÌõÑ ÏàòÎã®)
        if not ai_messages:
            content_messages = [msg for msg in messages if hasattr(msg, 'content') and msg.content and len(str(msg.content).strip()) > 50]
            if content_messages:
                # Í∞ÄÏû• Í∏¥ ÎÇ¥Ïö©ÏùÑ Í∞ÄÏßÑ Î©îÏãúÏßÄ ÏÑ†ÌÉù
                longest_msg = max(content_messages, key=lambda m: len(str(m.content)) if m.content else 0)
                ai_messages = [longest_msg]
                logger.info(f"   - Í∞ÄÏû• Í∏¥ ÏΩòÌÖêÏ∏† Î©îÏãúÏßÄ Ï∂îÏ∂ú: {len(str(longest_msg.content))}Ïûê")
        
        if ai_messages:
            last_ai_msg = ai_messages[-1]
            if hasattr(last_ai_msg, 'content') and last_ai_msg.content:
                final_content = str(last_ai_msg.content)
                logger.info(f"   - Ï∂îÏ∂úÎêú ÏµúÏ¢Ö ÏùëÎãµ Í∏∏Ïù¥: {len(final_content)}")
                logger.info(f"   - ÏµúÏ¢Ö ÏùëÎãµ ÎØ∏Î¶¨Î≥¥Í∏∞: {final_content[:200]}...")
            else:
                logger.warning(f"   - AI Î©îÏãúÏßÄÏóê contentÍ∞Ä ÏóÜÏùå: {type(last_ai_msg)}")
        else:
            logger.warning("   - AI Î©îÏãúÏßÄÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏùå")
            
            # ÎîîÎ≤ÑÍπÖÏùÑ ÏúÑÌï¥ Î™®Îì† Î©îÏãúÏßÄ ÎÇ¥Ïö© Ï∂úÎ†•
            logger.warning("   - Ï†ÑÏ≤¥ Î©îÏãúÏßÄ ÎÇ¥Ïö© Îç§ÌîÑ:")
            for i, msg in enumerate(messages):
                logger.warning(f"     Î©îÏãúÏßÄ {i}: {type(msg).__name__}")
                if hasattr(msg, 'content'):
                    content_str = str(msg.content)[:300] + "..." if len(str(msg.content)) > 300 else str(msg.content)
                    logger.warning(f"       ÎÇ¥Ïö©: {content_str}")
                else:
                    logger.warning(f"       ÎÇ¥Ïö©: content ÏÜçÏÑ± ÏóÜÏùå")
        
        # Ï∂îÍ∞Ä: final_stateÏóêÏÑú Îã§Î•∏ Î∞©Î≤ïÏúºÎ°ú ÏΩòÌÖêÏ∏† Ï∞æÍ∏∞
        if not final_content:
            logger.info("üîç ÎåÄÏïàÏ†Å ÏΩòÌÖêÏ∏† Ï∂îÏ∂ú ÏãúÎèÑ:")
            final_content = self._extract_content_from_state(final_state)
        
        # Ïó¨Ï†ÑÌûà ÏΩòÌÖêÏ∏†Í∞Ä ÏóÜÏúºÎ©¥ Ï¢ÖÌï© Î∂ÑÏÑù ÏÉùÏÑ±
        if not final_content or len(final_content.strip()) < 100:
            logger.info("üîç ÏΩòÌÖêÏ∏† Î∂ÄÏ°±ÏúºÎ°ú Ï¢ÖÌï© Î∂ÑÏÑù ÏÉùÏÑ±")
            final_content = self._generate_fallback_analysis(final_state)
        
        # ÏµúÏ¢Ö ÌôïÏù∏
        if final_content:
            logger.info(f"‚úÖ ÏµúÏ¢Ö ÏΩòÌÖêÏ∏† Ï∂îÏ∂ú ÏÑ±Í≥µ! Í∏∏Ïù¥: {len(final_content)}Ïûê")
        else:
            logger.error("‚ùå ÏµúÏ¢Ö ÏΩòÌÖêÏ∏† Ï∂îÏ∂ú Ïã§Ìå® - Î™®Îì† Î∞©Î≤ï Ïã§Ìå®")
            logger.error("‚ùå final_state Ï†ÑÏ≤¥ Îç§ÌîÑ:")
            import json
            try:
                state_dump = json.dumps(final_state, indent=2, default=str, ensure_ascii=False)[:2000]
                logger.error(f"   State dump (Ï≤òÏùå 2000Ïûê): {state_dump}")
            except Exception as dump_error:
                logger.error(f"   State dump Ïã§Ìå®: {dump_error}")
                logger.error(f"   State type: {type(final_state)}")
                logger.error(f"   State keys: {list(final_state.keys()) if hasattr(final_state, 'keys') else 'keys() ÏóÜÏùå'}")
        
        # Create analysis summary
        analysis_summary = final_content[:1000] + "..." if len(final_content) > 1000 else final_content
        
        # Log results
        logger.info(f"üìä Analysis result summary:")
        logger.info(f"  - Execution time: {execution_time:.2f} seconds")
        logger.info(f"  - Execution steps: {steps_count}")
        logger.info(f"  - Used tools: {tools_used}")
        logger.info(f"  - Tool results: {len(tool_results)}")
        logger.info(f"  - Analysis history: {len(analysis_history)}")
        logger.info(f"  - Final content: {len(final_content)} characters")
        
        # Preview final analysis content (for development)
        if final_content:
            preview = final_content[:500] + "..." if len(final_content) > 500 else final_content
            logger.info(f"üìù Final analysis content preview:\n{preview}")
        
        # Save final AI response to OpenSearch
        opensearch_saved = self._save_final_ai_response_to_opensearch(final_content, final_state)
        logger.info(f"üíæ OpenSearch Ï†ÄÏû• ÏµúÏ¢Ö Í≤∞Í≥º: {opensearch_saved}")
        
        # Save summary to Segments table
        segments_summary_saved = self._save_summary_to_segments_table(final_content)
        logger.info(f"üíæ Segments ÌÖåÏù¥Î∏î summary Ï†ÄÏû• ÏµúÏ¢Ö Í≤∞Í≥º: {segments_summary_saved}")
        
        # Extract references from final_state for streaming
        final_references = final_state.get('tool_references', [])
        logger.info(f"üìã ÏµúÏ¢Ö Ï∞∏Ï°∞ Í∞úÏàò: {len(final_references)}")
        
        return {
            'success': True,
            'document_id': self.document_id,
            'segment_id': self.segment_id,
            'segment_index': self.segment_index,
            'analysis_content': final_content,
            'analysis_summary': analysis_summary,
            'analysis_time': execution_time,
            'steps_count': steps_count,
            'tools_used': tools_used,
            'tool_results': tool_results,
            'analysis_history': analysis_history,
            'references': final_references,  # Ï∞∏Ï°∞ Ï†ïÎ≥¥ Ï∂îÍ∞Ä
            'opensearch_saved': opensearch_saved,  # Ïã§Ï†ú Ï†ÄÏû• Í≤∞Í≥º Ìè¨Ìï®
            'segments_summary_saved': segments_summary_saved,  # Segments ÌÖåÏù¥Î∏î summary Ï†ÄÏû• Í≤∞Í≥º
            'timestamp': datetime.now(timezone.utc).isoformat()
        }
    
    def _save_final_ai_response_to_opensearch(self, final_content: str, final_state: AgentState):
        """ÏµúÏ¢Ö AI ÏùëÎãµÏùÑ OpenSearchÏóê Ï†ÄÏû•"""
        logger.info("üîÑ _save_final_ai_response_to_opensearch Ìï®Ïàò Ìò∏Ï∂úÎê®")
        try:
            logger.info(f"üìù ÏµúÏ¢Ö ÏùëÎãµ Í∏∏Ïù¥: {len(final_content) if final_content else 0}")
            
            if not final_content or len(final_content.strip()) < 10:
                logger.info("üìù ÏùëÎãµ ÎÇ¥Ïö©Ïù¥ ÎÑàÎ¨¥ ÏßßÏïÑ ÏµúÏ¢Ö ÏùëÎãµ Ï†ÄÏû• Í±¥ÎÑàÎúÄ")
                return False
            
            logger.info(f"üîç OpenSearch ÏÑúÎπÑÏä§ ÏÉÅÌÉú: {self.opensearch_service is not None}")
            if not self.opensearch_service:
                logger.warning("‚ö†Ô∏è OpenSearch ÏÑúÎπÑÏä§Í∞Ä ÏóÜÏñ¥ ÏµúÏ¢Ö ÏùëÎãµ Ï†ÄÏû• Í±¥ÎÑàÎúÄ")
                return False
            
            # ÏÇ¨Ïö©Ïûê ÏøºÎ¶¨ Ï∂îÏ∂ú
            user_query = (f"Î¨∏ÏÑú '{self.document_id}' "
                         f"ÌéòÏù¥ÏßÄ {self.segment_index + 1}Î•º Îã§ÏñëÌïú Í∞ÅÎèÑÏóêÏÑú ÎèÑÍµ¨Î•º ÌôúÏö©ÌïòÏó¨ "
                         f"Î∂ÑÏÑùÌïòÍ≥† ÏÉÅÏÑ∏Ìûà ÏÑ§Î™ÖÌï¥Ï£ºÏÑ∏Ïöî.")
            logger.info(f"üí¨ ÏÇ¨Ïö©Ïûê ÏøºÎ¶¨: {user_query[:100]}...")
            
            # OpenSearch Ï†ÄÏû• ÏãúÎèÑ Ï†Ñ ÏÉÅÏÑ∏ Ï†ïÎ≥¥ Î°úÍ∑∏
            logger.info("üíæ OpenSearchÏóê ÏµúÏ¢Ö AI ÏùëÎãµ Ï†ÄÏû• ÏãúÎèÑ Ï§ë...")
            logger.info(f"   - segment_id: {self.segment_id}")
            logger.info(f"   - document_id: {self.document_id}")
            logger.info(f"   - segment_index: {self.segment_index}")
            logger.info(f"   - analysis_query Í∏∏Ïù¥: {len(user_query)}")
            logger.info(f"   - content Í∏∏Ïù¥: {len(final_content)}")
            logger.info(f"   - analysis_steps: final_ai_response")
            
            # segment-unit Î∞©ÏãùÏúºÎ°ú ÏµúÏ¢Ö AI ÏùëÎãµÏùÑ ai_analysis ÎèÑÍµ¨Î°ú Ï†ÄÏû•
            success = self.opensearch_service.add_ai_analysis_tool(
                index_id=self.index_id,
                document_id=self.document_id,
                segment_id=self.segment_id,
                segment_index=self.segment_index,
                analysis_query=user_query,
                content=final_content,
                analysis_steps="final_ai_response",
                analysis_type="final_ai_response",
                media_type=self.media_type
            )
            logger.info(f"üíæ OpenSearch Ï†ÄÏû• Í≤∞Í≥º: {success}")
            
            if success:
                logger.info(f"‚úÖ OpenSearch ÏµúÏ¢Ö AI ÏùëÎãµ Ï†ÄÏû• ÏôÑÎ£å")
                logger.info(f"üíæ Ï†ÄÏû•Îêú Îç∞Ïù¥ÌÑ∞: segment_id={self.segment_id}, query={user_query[:50]}...")
                
                # ÏûÑÎ≤†Îî© ÏóÖÎç∞Ïù¥Ìä∏ÎèÑ ÏãúÎèÑ
                try:
                    logger.info("üîÑ ÏûÑÎ≤†Îî© ÏóÖÎç∞Ïù¥Ìä∏ ÏãúÎèÑ Ï§ë...")
                    embedding_success = self.opensearch_service.update_segment_embeddings(self.index_id, self.segment_id)
                    logger.info(f"üîÑ ÏûÑÎ≤†Îî© ÏóÖÎç∞Ïù¥Ìä∏ Í≤∞Í≥º: {embedding_success}")
                except Exception as embedding_error:
                    logger.warning(f"‚ö†Ô∏è ÏûÑÎ≤†Îî© ÏóÖÎç∞Ïù¥Ìä∏ Ïã§Ìå® (Í≥ÑÏÜç ÏßÑÌñâ): {str(embedding_error)}")
                
                return True
            else:
                logger.error(f"‚ùå OpenSearch ÏµúÏ¢Ö AI ÏùëÎãµ Ï†ÄÏû• Ïã§Ìå® (success=False)")
                return False
            
        except Exception as e:
            logger.error(f"‚ùå OpenSearch ÏµúÏ¢Ö AI ÏùëÎãµ Ï†ÄÏû• Ïã§Ìå®: {str(e)}")
            logger.error(f"‚ùå ÏòàÏô∏ ÌÉÄÏûÖ: {type(e).__name__}")
            import traceback
            logger.error(f"‚ùå Ïò§Î•ò ÏÉÅÏÑ∏ Ïä§ÌÉù Ìä∏Î†àÏù¥Ïä§:")
            logger.error(traceback.format_exc())
            
            # Ï∂îÍ∞Ä ÎîîÎ≤ÑÍπÖ Ï†ïÎ≥¥
            logger.error("üîç Ï∂îÍ∞Ä ÎîîÎ≤ÑÍπÖ Ï†ïÎ≥¥:")
            logger.error(f"   - OpenSearch ÏÑúÎπÑÏä§ Í∞ùÏ≤¥: {type(self.opensearch_service) if self.opensearch_service else None}")
            logger.error(f"   - OpenSearch ÏóîÎìúÌè¨Ïù∏Ìä∏: {getattr(self.opensearch_service, 'endpoint', 'N/A') if self.opensearch_service else 'N/A'}")
            logger.error(f"   - OpenSearch Ïù∏Îç±Ïä§: {getattr(self.opensearch_service, 'index_name', 'N/A') if self.opensearch_service else 'N/A'}")
            logger.error(f"   - ÏÑ∏Í∑∏Î®ºÌä∏ Ï†ïÎ≥¥: segment_id={self.segment_id}")
            logger.error(f"   - ÏΩòÌÖêÏ∏† Ï†ïÎ≥¥: Í∏∏Ïù¥={len(final_content) if final_content else 0}")
            
            # ÏòàÏô∏Î•º Îã§Ïãú Î∞úÏÉùÏãúÌÇ§ÏßÄ ÏïäÍ≥† False Î∞òÌôòÏúºÎ°ú Ïã§Ìå®Î•º Î™ÖÌôïÌûà ÌëúÏãú
            return False
    
    def _save_summary_to_segments_table(self, final_content: str) -> bool:
        """Segments ÌÖåÏù¥Î∏îÏóê summaryÎ•º Ï†ÄÏû•"""
        logger.info("üîÑ _save_summary_to_segments_table Ìï®Ïàò Ìò∏Ï∂úÎê®")
        
        if not final_content or len(final_content.strip()) < 10:
            logger.info("üìù ÏöîÏïΩ ÎÇ¥Ïö©Ïù¥ ÎÑàÎ¨¥ ÏßßÏïÑ Ï†ÄÏû• Í±¥ÎÑàÎúÄ")
            return False
        
        if not self.dynamodb_service:
            logger.warning("‚ö†Ô∏è DynamoDB ÏÑúÎπÑÏä§Í∞Ä ÏóÜÏñ¥ summary Ï†ÄÏû• Í±¥ÎÑàÎúÄ")
            return False
        
        if not self.segment_id:
            logger.warning("‚ö†Ô∏è segment_idÍ∞Ä ÏóÜÏñ¥ summary Ï†ÄÏû• Í±¥ÎÑàÎúÄ")
            return False
        
        try:
            # Segments ÌÖåÏù¥Î∏îÏóê summary ÏóÖÎç∞Ïù¥Ìä∏
            logger.info(f"üíæ Segments ÌÖåÏù¥Î∏îÏóê summary Ï†ÄÏû• ÏãúÎèÑ Ï§ë...")
            logger.info(f"   - segment_id: {self.segment_id}")
            logger.info(f"   - summary Í∏∏Ïù¥: {len(final_content)}")
            
            # Summary ÎÇ¥Ïö© Í∏∏Ïù¥ Ï†úÌïú (DynamoDB ÏïÑÏù¥ÌÖú ÌÅ¨Í∏∞ Ï†úÌïú Í≥†Î†§)
            max_summary_length = 30000  # 30KB Ï†úÌïú
            if len(final_content) > max_summary_length:
                summary_content = final_content[:max_summary_length] + "...[ÏöîÏïΩÎê®]"
                logger.info(f"   - summary ÎÇ¥Ïö©Ïù¥ ÎÑàÎ¨¥ Í∏∏Ïñ¥ {max_summary_length}ÏûêÎ°ú Ï†úÌïúÎê®")
            else:
                summary_content = final_content
            
            # DynamoDB ÏóÖÎç∞Ïù¥Ìä∏ Îç∞Ïù¥ÌÑ∞ Íµ¨ÏÑ±
            update_data = {
                'summary': summary_content,
                'analysis_completed_at': datetime.now(timezone.utc).isoformat(),
                'updated_at': datetime.now(timezone.utc).isoformat()
            }
            
            # Segments ÌÖåÏù¥Î∏î ÏóÖÎç∞Ïù¥Ìä∏
            success = self.dynamodb_service.update_item(
                table_name='segments',
                key={'segment_id': self.segment_id},
                updates=update_data
            )
            
            if success:
                logger.info(f"‚úÖ Segments ÌÖåÏù¥Î∏îÏóê summary Ï†ÄÏû• ÏôÑÎ£å")
                logger.info(f"üíæ Ï†ÄÏû•Îêú Îç∞Ïù¥ÌÑ∞: segment_id={self.segment_id}, summary_length={len(summary_content)}")
                return True
            else:
                logger.error(f"‚ùå Segments ÌÖåÏù¥Î∏î summary Ï†ÄÏû• Ïã§Ìå® (success=False)")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Segments ÌÖåÏù¥Î∏î summary Ï†ÄÏû• Ïã§Ìå®: {str(e)}")
            logger.error(f"‚ùå ÏòàÏô∏ ÌÉÄÏûÖ: {type(e).__name__}")
            import traceback
            logger.error(f"‚ùå Ïò§Î•ò ÏÉÅÏÑ∏ Ïä§ÌÉù Ìä∏Î†àÏù¥Ïä§:")
            logger.error(traceback.format_exc())
            return False
    
    def _extract_content_from_state(self, final_state: AgentState) -> str:
        """final_stateÏóêÏÑú ÏΩòÌÖêÏ∏† Ï∂îÏ∂ú (Ïó¨Îü¨ Ï†ÑÎûµ ÏÇ¨Ïö©)"""
        logger.info("üîç _extract_content_from_state ÏãúÏûë")
        
        # analysis_historyÏóêÏÑú Ï∂îÏ∂ú ÏãúÎèÑ
        analysis_history = final_state.get('analysis_history', [])
        if analysis_history:
            logger.info(f"   - analysis_history Ìï≠Î™© Ïàò: {len(analysis_history)}")
            for item in reversed(analysis_history):  # ÏµúÍ∑º Í≤ÉÎ∂ÄÌÑ∞
                if isinstance(item, dict):
                    for key in ['content', 'result', 'output', 'analysis']:
                        if key in item:
                            content_candidate = str(item[key])
                            if len(content_candidate.strip()) > 50:
                                logger.info(f"   - analysis_history['{key}']ÏóêÏÑú ÏΩòÌÖêÏ∏† Ï∂îÏ∂ú: {len(content_candidate)}Ïûê")
                                return content_candidate
        
        # tool_resultsÏóêÏÑú Ï∂îÏ∂ú ÏãúÎèÑ
        tool_results = final_state.get('tool_results', [])
        if tool_results:
            logger.info(f"   - tool_results Ìï≠Î™© Ïàò: {len(tool_results)}")
            for result in reversed(tool_results):  # ÏµúÍ∑º Í≤ÉÎ∂ÄÌÑ∞
                if isinstance(result, dict):
                    for key in ['result', 'content', 'output', 'analysis']:
                        if key in result:
                            content_candidate = str(result[key])
                            if len(content_candidate.strip()) > 50:
                                logger.info(f"   - tool_results['{key}']ÏóêÏÑú ÏΩòÌÖêÏ∏† Ï∂îÏ∂ú: {len(content_candidate)}Ïûê")
                                return content_candidate
        
        # combined_analysis_contextÏóêÏÑú Ï∂îÏ∂ú
        combined_context = final_state.get('combined_analysis_context', '')
        if combined_context and len(combined_context.strip()) > 50:
            logger.info(f"   - combined_analysis_contextÏóêÏÑú ÏΩòÌÖêÏ∏† Ï∂îÏ∂ú: {len(combined_context)}Ïûê")
            return combined_context
        
        # final_stateÏùò Î™®Îì† Í∞íÏóêÏÑú Í∏¥ Î¨∏ÏûêÏó¥ Ï∞æÍ∏∞ (ÏµúÌõÑ ÏàòÎã®)
        logger.info("üîç final_state Ï†ÑÏ≤¥ÏóêÏÑú ÏΩòÌÖêÏ∏† Ï∞æÍ∏∞:")
        for key, value in final_state.items():
            if isinstance(value, str) and len(value.strip()) > 100:
                logger.info(f"   - '{key}' ÌÇ§ÏóêÏÑú ÏΩòÌÖêÏ∏† Î∞úÍ≤¨: {len(value)}Ïûê")
                return value
            elif isinstance(value, list) and value:
                # Î¶¨Ïä§Ìä∏Ïùò ÎßàÏßÄÎßâ Ìï≠Î™©Ïù¥ Î¨∏ÏûêÏó¥Ïù∏ÏßÄ ÌôïÏù∏
                last_item = value[-1]
                if isinstance(last_item, str) and len(last_item.strip()) > 100:
                    logger.info(f"   - '{key}' Î¶¨Ïä§Ìä∏Ïùò ÎßàÏßÄÎßâ Ìï≠Î™©ÏóêÏÑú ÏΩòÌÖêÏ∏† Î∞úÍ≤¨: {len(last_item)}Ïûê")
                    return last_item
                elif isinstance(last_item, dict) and 'content' in last_item:
                    content_candidate = str(last_item['content'])
                    if len(content_candidate.strip()) > 100:
                        logger.info(f"   - '{key}' Î¶¨Ïä§Ìä∏ Ìï≠Î™©Ïùò contentÏóêÏÑú Î∞úÍ≤¨: {len(content_candidate)}Ïûê")
                        return content_candidate
        
        logger.info("   - Ï∂îÏ∂ú Í∞ÄÎä•Ìïú ÏΩòÌÖêÏ∏†Î•º Ï∞æÏßÄ Î™ªÌï®")
        return ""
    
    def _generate_fallback_analysis(self, final_state: AgentState) -> str:
        """ÏΩòÌÖêÏ∏† Ï∂îÏ∂úÏóê Ïã§Ìå®ÌñàÏùÑ Îïå fallback Î∂ÑÏÑù ÏÉùÏÑ±"""
        logger.info("üìù _generate_fallback_analysis ÏãúÏûë")
        
        # Í∏∞Î≥∏ Ï†ïÎ≥¥ ÏàòÏßë
        document_id = final_state.get('document_id', self.document_id)
        segment_id = final_state.get('segment_id', self.segment_id)
        user_query = final_state.get('user_query', '')
        current_step = final_state.get('current_step', 0)
        
        # StateÏóêÏÑú Ïú†Ïö©Ìïú Ï†ïÎ≥¥ ÏàòÏßë
        analysis_history = final_state.get('analysis_history', [])
        tool_results = final_state.get('tool_results', [])
        tools_used = final_state.get('tools_used', [])
        combined_context = final_state.get('combined_analysis_context', '')
        
        # Ïã§ÌñâÎêú ÎèÑÍµ¨ Ï†ïÎ≥¥
        tools_info = []
        for entry in analysis_history:
            if isinstance(entry, dict):
                tool_name = entry.get('tool_name', 'Unknown')
                success = entry.get('success', False)
                result_preview = str(entry.get('result', ''))[:200] + "..." if entry.get('result') else "No result"
                tools_info.append({
                    'tool': tool_name,
                    'success': success,
                    'preview': result_preview
                })
        
        # Fallback Î∂ÑÏÑù ÏÉùÏÑ±
        fallback_analysis = f"""# Î¨∏ÏÑú Î∂ÑÏÑù Í≤∞Í≥º (Fallback ÏÉùÏÑ±)

## üìã Î∂ÑÏÑù Í∞úÏöî
- **Î¨∏ÏÑú ID**: {document_id}
- **ÏÑ∏Í∑∏Î®ºÌä∏ ID**: {segment_id}
- **Î∂ÑÏÑù ÏöîÏ≤≠**: {user_query[:300]}{'...' if len(user_query) > 300 else ''}
- **Ïã§Ìñâ Îã®Í≥Ñ**: {current_step}
- **ÏÉÅÌÉú**: Î∂ÑÏÑù ÏôÑÎ£å (Fallback ÏÉùÏÑ±)

## üõ†Ô∏è Ïã§ÌñâÎêú Î∂ÑÏÑù ÎèÑÍµ¨
"""
        
        if tools_info:
            for i, tool_info in enumerate(tools_info, 1):
                status = "‚úÖ" if tool_info['success'] else "‚ùå"
                fallback_analysis += f"""
### {i}. {tool_info['tool']} {status}
- **Ïã§Ìñâ ÏÉÅÌÉú**: {"ÏÑ±Í≥µ" if tool_info['success'] else "Ïã§Ìå®"}
- **Í≤∞Í≥º ÎØ∏Î¶¨Î≥¥Í∏∞**: {tool_info['preview']}
"""
        else:
            fallback_analysis += "\n- Ïã§ÌñâÎêú ÎèÑÍµ¨Í∞Ä ÏóÜÏäµÎãàÎã§.\n"
        
        # Ï¢ÖÌï© Î∂ÑÏÑù Ïª®ÌÖçÏä§Ìä∏Í∞Ä ÏûàÎäî Í≤ΩÏö∞ Ìè¨Ìï®
        if combined_context and len(combined_context.strip()) > 10:
            fallback_analysis += f"""

## üîó ÏàòÏßëÎêú Î∂ÑÏÑù Ï†ïÎ≥¥
{combined_context[:800]}{'...' if len(combined_context) > 800 else ''}
"""
        
        fallback_analysis += f"""

## ‚ö†Ô∏è Î∂ÑÏÑù ÏôÑÎ£å Ï†ïÎ≥¥
Ïù¥ Í≤∞Í≥ºÎäî ÏãúÏä§ÌÖúÏóêÏÑú ÏûêÎèôÏúºÎ°ú ÏÉùÏÑ±Îêú fallback Î∂ÑÏÑùÏûÖÎãàÎã§. 
ÏõêÎ≥∏ AI Î™®Îç∏Ïùò ÏùëÎãµÏùÑ Ï∂îÏ∂úÌï† Ïàò ÏóÜÏñ¥ ÏàòÏßëÎêú Ï†ïÎ≥¥Î•º Î∞îÌÉïÏúºÎ°ú Íµ¨ÏÑ±ÎêòÏóàÏäµÎãàÎã§.

- **Ï¥ù Ïã§Ìñâ Îã®Í≥Ñ**: {current_step}
- **ÏÇ¨Ïö©Îêú ÎèÑÍµ¨ Ïàò**: {len(tools_info)}
- **ÏàòÏßëÎêú Ï†ïÎ≥¥Îüâ**: {len(combined_context)} Î¨∏Ïûê

---
*Î∂ÑÏÑù ÏÉùÏÑ± ÏãúÍ∞Ñ: {datetime.now(timezone.utc).isoformat()}*
*ÏÉùÏÑ± Î∞©Ïãù: Fallback Analysis*
"""
        
        logger.info(f"üìù Fallback Î∂ÑÏÑù ÏÉùÏÑ± ÏôÑÎ£å - Í∏∏Ïù¥: {len(fallback_analysis)} Î¨∏Ïûê")
        logger.info(f"üìù Ìè¨Ìï®Îêú ÎèÑÍµ¨ Ï†ïÎ≥¥: {len(tools_info)}Í∞ú")
        
        return fallback_analysis
"""
AWS IDP AI Analysis - ReAct Analysis Lambda Function
"""

import json
import os
import sys
import logging
import time
import asyncio
import boto3
from datetime import datetime, timezone
from typing import Dict, Any
from agent.analysis_agent import AnalysisAgent

# Add Local Module Path for Lambda Environment
sys.path.append('/opt/python')
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Common module imports
from common import (
    DynamoDBService,
    S3Service,
    AWSClientFactory,
    setup_logging,
    get_current_timestamp,
)

# Logging Setup
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# Initialize common services
db_service = DynamoDBService()
s3_service = S3Service()
aws_clients = AWSClientFactory()

# Environment Variables
STAGE = os.environ.get('STAGE', 'prod')
S3_BUCKET_NAME = os.environ.get('DOCUMENTS_TABLE_NAME')
SEGMENTS_TABLE_NAME = os.environ.get('SEGMENTS_TABLE_NAME')
MODEL_REGION = os.environ.get('AWS_REGION', 'us-west-2')  # Lambdaì—ì„œ ìë™ ì œê³µ
BEDROCK_AGENT_MODEL_ID = os.environ.get('BEDROCK_AGENT_MODEL_ID', 'us.anthropic.claude-3-7-sonnet-20250219-v1:0')
BEDROCK_AGENT_MAX_TOKENS = int(os.environ.get('BEDROCK_AGENT_MAX_TOKENS', 8192))
OPENSEARCH_ENDPOINT = os.environ.get('OPENSEARCH_ENDPOINT', '')
DOCUMENTS_TABLE_NAME = os.environ.get('DOCUMENTS_TABLE_NAME')

class LambdaReActAnalysisHandler:
    """Handler for LangGraph-based ReAct analysis"""
    
    def __init__(self):
        """Initialize the handler"""
        self.model_id = BEDROCK_AGENT_MODEL_ID
        self.max_tokens = BEDROCK_AGENT_MAX_TOKENS
        
        logger.info(f"âœ… ReAct analysis handler initialized")
        logger.info("-" * 50)
        logger.info(f"Agent Model: {self.model_id}")
        logger.info(f"Agent Max Tokens: {self.max_tokens}")
        logger.info(f"OpenSearch: {'Enabled' if OPENSEARCH_ENDPOINT else 'Disabled'}")
        logger.info("-" * 50)
    

    async def _analyze_single_segment(self, segment_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        LangGraph-based single segment analysis
        """
        try:
            index_id = segment_data['index_id']
            document_id = segment_data['document_id']
            segment_index = segment_data['segment_index']
            segment_id = segment_data['segment_id']
            image_uri = segment_data['image_uri']
            file_path = segment_data['file_path']
            thread_id = segment_data.get('thread_id')  # ì¶”ê°€: thread_id ì¶”ì¶œ
            
            # ë™ì˜ìƒ ì±•í„° ì •ë³´ ì¶”ì¶œ
            segment_type = segment_data.get('segment_type')
            start_timecode_smpte = segment_data.get('start_timecode_smpte')
            end_timecode_smpte = segment_data.get('end_timecode_smpte')
            
            # Step Functionì—ì„œ ì „ë‹¬ë°›ì€ media_type ì¶”ì¶œ
            media_type = segment_data.get('media_type', 'DOCUMENT')
            
            # Start logging
            logger.info(f"ğŸ¯ Segment analysis started:")
            logger.info("-" * 50)
            logger.info(f"Index ID: {index_id}")
            logger.info(f"Document ID: {document_id}")
            logger.info(f"Media Type: {media_type}")
            logger.info(f"Segment Index: {segment_index}")
            logger.info(f"Segment ID: {segment_id}")
            logger.info(f"Segment Type: {segment_type}")
            logger.info(f"Image URI: {image_uri}")
            logger.info(f"File Path: {file_path}")
            logger.info(f"Start Timecode: {start_timecode_smpte}")
            logger.info(f"End Timecode: {end_timecode_smpte}")
            logger.info(f"Thread ID: {thread_id}")  # ì¶”ê°€: thread_id ë¡œê¹…
            logger.info("-" * 50)

            # Update segment analysis status
            self._update_segment_analysis_status(
                document_id=document_id, 
                segment_id=segment_id,
                segment_index=segment_index,
                steps_count=0,
                status='react_analyzing',
                analysis_summary='LangGraph analysis started'
            )
            
            # ReAct Analysis Start Logging
            start_time = time.time()
            
            # User Query
            user_query = f"'{document_id}' ì„¸ê·¸ë¨¼íŠ¸ {segment_index + 1}ë¥¼ ë‹¤ì–‘í•œ ê°ë„ì™€ ì‹œê°ì—ì„œ ë„êµ¬ë¥¼ ìµœëŒ€í•œ í™œìš©í•˜ì—¬ ë¶„ì„í•´ì£¼ê³ , ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ ì •ë³´ì™€ ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ì„¸ê·¸ë¨¼íŠ¸ì— ìˆëŠ” ë‚´ìš©ì„ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”."
            
            logger.info("-" * 50)
            logger.info(f"ğŸ’¬ User Query: {user_query}")
            logger.info("-" * 50)

            agent = AnalysisAgent(
                index_id=index_id,
                document_id=document_id,
                segment_id=segment_id,
                segment_index=segment_index,
                image_uri=image_uri, 
                file_path=file_path,
                model_id=self.model_id,
                max_tokens=self.max_tokens,
                thread_id=thread_id,  # ì¶”ê°€: thread_id ì „ë‹¬
                segment_type=segment_type,  # ë™ì˜ìƒ ì±•í„° íƒ€ì…
                start_timecode_smpte=start_timecode_smpte,  # ë™ì˜ìƒ ì±•í„° ì‹œì‘ ì‹œê°„
                end_timecode_smpte=end_timecode_smpte,  # ë™ì˜ìƒ ì±•í„° ì¢…ë£Œ ì‹œê°„
                media_type=media_type  # Documents í…Œì´ë¸”ì˜ media_type
            )

            logger.info("âš¡ ReAct Analysis Running...")
            analysis_result = agent.analyze_document(
                user_query=user_query,
                analysis_type="comprehensive"
            )
            
            analysis_time = time.time() - start_time
            logger.info("-" * 50)
            logger.info(f"â±ï¸ Total Analysis Time: {analysis_time:.2f} seconds")
            logger.info("-" * 50)

            print(f"analysis_result: {analysis_result}")
            
            # Analysis Result Processing
            if analysis_result.get('success'):
                logger.info("ğŸ‰" + "=" * 100)
                logger.info("ğŸ‰ ReAct Analysis Success!")
                logger.info("ğŸ‰" + "=" * 100)
                logger.info(f"ğŸ”¢ Steps Executed: {analysis_result.get('steps_count', 0)}")
                logger.info(f"ğŸ› ï¸ Tools Used: {analysis_result.get('tools_used', [])}")
                logger.info(f"â±ï¸ Total Analysis Time: {analysis_time:.2f} seconds")
                
                # Analysis Summary
                analysis_content = analysis_result.get('analysis_content', '')
                analysis_summary = analysis_result.get('analysis_summary', '')
                
                logger.info(f"ğŸ“ Analysis Summary Character Count: {len(analysis_summary):,}")
                logger.info(f"ğŸ“„ Total Analysis Content Character Count: {len(analysis_content):,}")
                
                # Tool Result Details
                tool_results = analysis_result.get('tool_results', [])
                if tool_results:
                    logger.info(f"ğŸ”§ Tool Result Details:")
                    for i, tool_result in enumerate(tool_results[:3]):
                        tool_name = tool_result.get('tool_name', 'Unknown')
                        success = tool_result.get('success', False)
                        message = tool_result.get('message', '')
                        status = "âœ…" if success else "âŒ"
                        logger.info(f"  {i+1}. {status} {tool_name}: {message[:100]}...")
                
                if not analysis_summary and analysis_content:
                    # Only use content if summary is not available for error handling
                    analysis_summary = analysis_content[:1000] + "..." if len(analysis_content) > 1000 else analysis_content
                
                # Update DynamoDB segment ReAct analysis status
                try:
                    self._update_segment_analysis_status(
                        document_id=document_id,
                        segment_id=segment_id,
                        segment_index=segment_index,
                        steps_count=analysis_result.get('steps_count', 0),
                        status='completed',
                        analysis_summary=analysis_summary
                    )
                except Exception as e:
                    logger.error(f"âŒ DynamoDB ReAct analysis status update failed: {str(e)}")

                # Note: The new LangGraph-based agent automatically saves tool results and final responses to OpenSearch
                
                # OpenSearch ì €ì¥ ìƒíƒœ í™•ì¸ ë° ë¡œê¹…
                opensearch_saved = analysis_result.get('opensearch_saved', False)
                logger.info(f"ğŸ’¾ OpenSearch ìµœì¢… ì‘ë‹µ ì €ì¥ ìƒíƒœ: {opensearch_saved}")
                
                if not opensearch_saved:
                    logger.warning("âš ï¸ ìµœì¢… AI ì‘ë‹µì´ OpenSearchì— ì €ì¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
                    logger.warning("âš ï¸ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì—¬ ì €ì¥ ì‹¤íŒ¨ ì›ì¸ì„ íŒŒì•…í•˜ì„¸ìš”.")
                else:
                    logger.info("âœ… ìµœì¢… AI ì‘ë‹µì´ OpenSearchì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                
                logger.info("ğŸ‰" + "=" * 80)
                logger.info(f"ğŸ‰ Segment {segment_index} ReAct analysis completed!")
                logger.info("ğŸ‰" + "=" * 80)
                
                # References ì •ë³´ ì¶”ì¶œ ë° ìŠ¤íŠ¸ë¦¬ë° ì¤€ë¹„
                references = analysis_result.get('references', [])
                logger.info(f"ğŸ“‹ ì¶”ì¶œëœ ì°¸ì¡° ê°œìˆ˜: {len(references)}")
                
                # Referencesê°€ ìˆëŠ” ê²½ìš° ë¡œê¹…
                if references:
                    for i, ref in enumerate(references, 1):
                        title = ref.get('title', f'ì°¸ì¡° {i}')
                        logger.info(f"ğŸ“‹ ì°¸ì¡° {i}: {title}")
                
                return {
                    'success': True,
                    'document_id': document_id,
                    'segment_id': segment_id,
                    'segment_index': segment_index,
                    'analysis_summary': analysis_summary,
                    'analysis_content': analysis_content,
                    'analysis_time': analysis_time,
                    'steps_count': analysis_result.get('steps_count', 0),
                    'tools_used': analysis_result.get('tools_used', []),
                    'tool_results': analysis_result.get('tool_results', []),
                    'analysis_history': analysis_result.get('analysis_history', []),
                    'references': references,  # ì°¸ì¡° ì •ë³´ ì¶”ê°€
                    'opensearch_saved': opensearch_saved,  # ì‹¤ì œ ì €ì¥ ìƒíƒœ í¬í•¨
                    'timestamp': get_current_timestamp()
                }
            else:
                error_msg = analysis_result.get('error', 'Unknown analysis error')
                logger.error(f"âŒ ReAct analysis failed: {error_msg}")
                raise Exception(f"Analysis failed: {error_msg}")
                
        except Exception as e:
            error_msg = f"Segment analysis processing failed: {str(e)}"
            logger.error(error_msg)
            logger.error(f"Exception details: {str(e)}")
            
            # Stack trace logging
            import traceback
            logger.error("ğŸ’¥ Stack trace:")
            logger.error(traceback.format_exc())
            
            # Update segment ReAct analysis status to 'react_failed' when an error occurs
            try:
                self._update_segment_analysis_status(
                    document_id=segment_data['document_id'],
                    segment_id=segment_data['segment_id'],
                    segment_index=segment_data['segment_index'],
                    steps_count=0,
                    status='react_failed',
                    analysis_summary=f'ReAct analysis failed: {str(e)[:200]}'
                )
            except Exception as update_error:
                logger.error(f"[DynamoDB] ReAct analysis status update failed: {str(update_error)}")
            
            raise Exception(error_msg)
    
    def _update_segment_analysis_status(self, document_id: str, segment_id: str, segment_index: int, 
                                   steps_count: int, status: str, analysis_summary: str):
        """Update DynamoDB Segments table status"""
        try:
            if not (SEGMENTS_TABLE_NAME):
                logger.warning("SEGMENTS_TABLE_NAME environment variable is not set")
                return
            
            # Construct update expression based on status
            if status == 'react_analyzing':
                # ReAct analysis started
                update_expr = """
                    SET #status = :status,
                        updated_at = :updated_at
                """
                attr_values = {
                    ':status': status,
                    ':updated_at': get_current_timestamp()
                }
                attr_names = { '#status': 'status' }
            elif status == 'react_completed':
                # ReAct analysis completed
                update_expr = """
                    SET #status = :status,
                        analysis_completed_at = :analysis_completed_at,
                        analysis_error = :analysis_error,
                        analysis_failed_at = :analysis_failed_at,
                        steps_count = :steps_count,
                        updated_at = :updated_at
                """
                attr_values = {
                    ':status': status,
                    ':analysis_completed_at': get_current_timestamp(),
                    ':analysis_error': '', 
                    ':analysis_failed_at': '',
                    ':steps_count': steps_count,
                    ':updated_at': get_current_timestamp()
                }
                attr_names = { '#status': 'status' }
            elif status == 'react_failed':
                # ReAct analysis failed
                update_expr = """
                    SET #status = :status,
                        analysis_completed_at = :analysis_completed_at,
                        analysis_error = :analysis_error,
                        analysis_failed_at = :analysis_failed_at,
                        updated_at = :updated_at
                """
                attr_values = {
                    ':status': status,
                    ':analysis_completed_at': '',
                    ':analysis_error': analysis_summary[:500] if analysis_summary else '',  # ì˜¤ë¥˜ ë©”ì‹œì§€
                    ':analysis_failed_at': get_current_timestamp(),
                    ':updated_at': get_current_timestamp()
                }
                attr_names = { '#status': 'status' }
            else:
                # Default update
                update_expr = """
                    SET #status = :status,
                        updated_at = :updated_at
                """
                attr_values = {
                    ':status': status,
                    ':updated_at': get_current_timestamp()
                }
                attr_names = { '#status': 'status' }
            
            # DynamoDB update using common service
            db_service.update_item(
                'segments',
                {'segment_id': segment_id},
                update_expression=update_expr,
                expression_attribute_values=attr_values,
                expression_attribute_names=attr_names
            )
            
            logger.info(f"[DynamoDB] Segment ReAct analysis status updated - segment_id: {segment_id}, status: {status}")
            
        except Exception as e:
            logger.error(f"[DynamoDB] Segment ReAct analysis status update failed - segment_id: {segment_id}, error: {str(e)}")
            raise

# Global handler instance
handler_instance = None

def lambda_handler(event, context):
    """
    Lambda handler (AWS Lambda entry point)
    
    Args:
        event: Step Function event
        context: Lambda context
        
    Returns:
        Analysis result
    """
    global handler_instance
    handler_start_time = datetime.now(timezone.utc)
    
    try:
        # ğŸ” Lambda handler start logging
        logger.info("ğŸš€" + "=" * 100)
        logger.info("ğŸš€ Start ReAct Analysis Handler")
        logger.info("ğŸš€" + "=" * 100)

        # ğŸ” Environment variables logging
        logger.info("ğŸŒ Environment variables:")
        logger.info("-" * 50)
        important_env_vars = [
            'STAGE', 'BEDROCK_AGENT_MODEL_ID', 'BEDROCK_AGENT_MAX_TOKENS', 
            'BEDROCK_IMAGE_MODEL_ID', 'BEDROCK_IMAGE_MAX_TOKENS',
            'AWS_REGION', 
            'OPENSEARCH_ENDPOINT'
        ]
        for var in important_env_vars:
            value = os.environ.get(var, 'NOT_SET')
            if len(str(value)) > 50:
                value = str(value)[:50] + "..."
            logger.info(f"   {var}: {value}")
        logger.info("-" * 50)
        
        # ğŸ” Input event logging
        logger.info("ğŸ“‹ Input event:")
        logger.info("-" * 50)
        logger.info(json.dumps(event, ensure_ascii=False, indent=2, default=str))
        logger.info("-" * 50)
        
        # Initialize handler instance
        if handler_instance is None:
            handler_instance = LambdaReActAnalysisHandler()
        
        # Run asynchronous analysis
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        try:
            # Convert event from Step Function Map to _analyze_single_segment format
            segment_data = {
                'index_id': event['index_id'],
                'document_id': event['document_id'],
                'segment_index': event['segment_index'],
                'segment_id': event['segment_id'],
                'image_uri': event['image_uri'],
                'file_path': event.get('file_uri', ''),
                'thread_id': event.get('thread_id'),  # ì¶”ê°€: thread_id ì „ë‹¬
                # ë™ì˜ìƒ ì±•í„° ì •ë³´ ì¶”ê°€
                'segment_type': event.get('segment_type'),
                'start_timecode_smpte': event.get('start_timecode_smpte'),
                'end_timecode_smpte': event.get('end_timecode_smpte'),
            }
            
            logger.info(f"ğŸ”„ Extracted thread_id from event: {segment_data.get('thread_id')}")
            
            result = loop.run_until_complete(handler_instance._analyze_single_segment(segment_data))
        finally:
            loop.close()
        
        # ğŸ” Lambda handler completion logging
        handler_end_time = datetime.now(timezone.utc)
        handler_duration = (handler_end_time - handler_start_time).total_seconds()
        
        logger.info("ğŸš€" + "=" * 100)
        logger.info(f"ğŸ¯ ReAct Lambda handler completed - Segment {result.get('segment_index', 'unknown')}")
        logger.info("ğŸš€" + "=" * 100)
        logger.info(f"â° Lambda end time: {handler_end_time.isoformat()}")
        logger.info(f"âŒ› Total processing time: {handler_duration:.2f} seconds")
        logger.info(f"âœ… Segment processing success: {result.get('success', False)}")
        logger.info(f"ğŸ“ƒ Processed segment: {result.get('segment_index', 'unknown')}")
        logger.info(f"ğŸ“ Index ID: {result.get('index_id', 'unknown')}")
        logger.info(f"ğŸ“„ Document ID: {result.get('document_id', 'unknown')}")
        logger.info(f"ğŸ”¢ ReAct steps: {result.get('steps_count', 0)}")
        logger.info(f"ğŸ› ï¸ Used tools: {result.get('tools_used', [])}")
        logger.info(f"ğŸ“„ Analysis time: {result.get('analysis_time', 0):.2f} seconds")
        logger.info(f"ğŸ” OpenSearch saved: {result.get('opensearch_saved', False)}")
        
        if result.get('success'):
            summary_length = len(result.get('analysis_summary', ''))
            logger.info(f"ğŸ“ Analysis summary character count: {summary_length}")
            logger.info("ğŸ‰ ReAct analysis completed successfully!")
        else:
            logger.error(f"âŒ Segment {result.get('segment_index', 'unknown')} analysis failed")
        
        logger.info("ğŸš€" + "=" * 100)
        return result
        
    except Exception as e:
        # ğŸ” Overall error handling logging
        handler_end_time = datetime.now(timezone.utc)
        handler_duration = (handler_end_time - handler_start_time).total_seconds()
        
        logger.error("ğŸš€" + "=" * 100)
        logger.error("ğŸ’¥ ReAct analysis Lambda handler error")
        logger.error("ğŸš€" + "=" * 100)
        logger.error(f"â° Error occurred time: {handler_end_time.isoformat()}")
        logger.error(f"âŒ› Processing time before error: {handler_duration:.2f} seconds")
        logger.error(f"ğŸ’¥ Error message: {str(e)}")
        logger.error(f"ğŸ’¥ Error type: {type(e).__name__}")
        logger.error(f"ğŸ“¨ Request ID: {context.aws_request_id}")
        
        # Stack trace logging
        import traceback
        logger.error("ğŸ’¥ Stack trace:")
        logger.error(traceback.format_exc())
        
        # Create failure response
        error_response = {
            'success': False,
            'error': str(e),
            'error_details': {
                'error_type': type(e).__name__,
                'processing_time': handler_duration,
                'timestamp': get_current_timestamp(),
                'function_name': context.function_name,
                'request_id': context.aws_request_id
            },
            'index_id': event.get('index_id', 'unknown'),
            'document_id': event.get('document_id', 'unknown'),
            'segment_id': event.get('segment_id', 'unknown'),
            'segment_index': event.get('segment_index', 0),
            'timestamp': get_current_timestamp()
        }
        
        logger.error(f"ğŸ’¥ Error response: {json.dumps(error_response, indent=2, ensure_ascii=False, default=str)}")
        logger.error("ğŸš€" + "=" * 100)
        
        # Raise exception to handle failure in Step Function
        raise Exception(f"Lambda execution failed: {str(e)}")
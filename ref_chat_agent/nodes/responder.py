# src/agent/nodes/responder.py
import time
import re
from colorama import Fore, Style
from typing import cast, List, Dict, Any, Optional, Tuple
from langchain_core.messages import HumanMessage, AIMessage
from src.chat_agent.states.schemas import AgentState
from src.chat_agent.prompts import prompt_manager
from src.chat_agent.tools.base import Reference

class ResponderNode:
    """
    ResponderNode generates comprehensive responses based on executed tasks 
    and direct responses, handling thought process display as needed.
    """
    
    def __init__(self, llm, show_thought_process: bool = False):
        """
        Initialize the ResponderNode
        
        Args:
            llm: LLM service to use for response generation
            show_thought_process: Whether to show the thought process in the output
        """
        self.model = llm.model
        self.show_thought_process = show_thought_process
        
        # Set the variant based on the show_thought_process parameter
        prompt_manager.toggle_variant("responder", "with_thought_process", show_thought_process)
    
    def toggle_thought_process(self, show: bool):
        """
        Toggle whether to show the thought process in the output
        
        Args:
            show: Whether to show the thought process
        """
        self.show_thought_process = show
        prompt_manager.toggle_variant("responder", "with_thought_process", show)
        return self
    
    def _build_context(self, state: AgentState) -> Tuple[str, List[str], List[Any]]:
        """
        Build context and references from state
        
        Args:
            state: The current agent state
            
        Returns:
            Tuple containing:
              - context text (str)
              - list of reference strings (List[str])
              - list of original references (List[Any])
        """
        if state.plan.direct_response:
            return f"Direct response: {state.plan.direct_response}", [], []
        
        # Initialize references and context parts
        references = []
        context_parts = [f"Plan overview: {state.plan.overview}\n\nExecuted tasks and their results:\n"]
        ref_index = 1
        
        # Store original references
        original_references = []
        
        for task_info in state.executed_tasks:
            task = task_info['task']
            result = task_info.get('result', '')
            success = task_info.get('success', False)

            if not success:
                context_parts.append(f"Status: Failed")
                context_parts.append(f"Error: {result}")
                continue

            # Process the result
            if isinstance(result, dict):
                # Add LLM text if available
                if 'llm_text' in result:
                    context_parts.append(result['llm_text'])
                
                # Process references
                if 'references' in result and isinstance(result['references'], list):
                    # Store original references
                    original_references.extend(result['references'])
                    
                    for ref in result['references']:
                        # Format reference for display
                        ref_text = f"[{ref_index}] {ref.get('type', 'link')}: {ref.get('value', '')}"
                        if ref.get('title'):
                            ref_text += f" - {ref.get('title')}"
                        references.append(ref_text)
                        ref_index += 1
                # Fallback if no LLM text or specific processing
                if 'llm_text' not in result and 'results' not in result:
                    context_parts.append(str(result))
            else:
                context_parts.append(str(result))

        return "\n".join(context_parts), references, original_references
    
    def _filter_references_from_response(self, response: str, original_references: List[Any]) -> List[Any]:
        """
        Filter references that appear in the final response
        
        Args:
            response: The final response text
            original_references: List of all original references
            
        Returns:
            List of references that are referenced in the response
        """
        if not original_references:
            print(Fore.CYAN + "\nğŸ“‹ ì°¸ì¡° ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤." + Style.RESET_ALL)
            return []
        
        # ì‘ë‹µ ë¡œê¹… (ë””ë²„ê¹…ìš©)
        print(Fore.GREEN + "\nğŸ“ í•„í„°ë§ ëŒ€ìƒ ì‘ë‹µ ë‚´ìš©:" + Style.RESET_ALL)
        print("=" * 80)
        # ì‘ë‹µì´ ë„ˆë¬´ ê¸¸ë©´ ì²˜ìŒ 500ìì™€ ë§ˆì§€ë§‰ 500ìë§Œ ì¶œë ¥
        if len(response) > 1000:
            print(response[:500] + "\n...\n" + response[-500:])
        else:
            print(response)
        print("=" * 80 + "\n")
        
        print(Fore.CYAN + f"\nğŸ“‹ ì›ë³¸ ì°¸ì¡° ë°ì´í„° {len(original_references)}ê°œ ë°œê²¬ë¨:" + Style.RESET_ALL)
        for i, ref in enumerate(original_references):
            ref_type = ref.get('type', 'ì•Œ ìˆ˜ ì—†ìŒ')
            ref_title = ref.get('title', 'ì œëª© ì—†ìŒ')
            ref_value = ref.get('value', '')
            value_preview = ref_value[:30] + '...' if ref_value and len(ref_value) > 30 else ref_value
            print(f"  {i+1}. [{ref_type}] {ref_title} - {value_preview}")
        
        # ì´ë¯¸ì§€ ì°¸ì¡° íŒŒì•… (ì´ì œ ìë™ í¬í•¨í•˜ì§€ ì•ŠìŒ)
        image_references = [ref for ref in original_references if ref.get('type') == 'image']
        if image_references:
            print(Fore.CYAN + f"\nğŸ–¼ï¸ ì´ë¯¸ì§€ ì°¸ì¡° {len(image_references)}ê°œ ê°ì§€ë¨ (í•„í„°ë§ ëŒ€ìƒ)" + Style.RESET_ALL)
        
        # ì°¸ì¡° ê°ì²´ ID ë˜ëŠ” ì œëª©ì—ì„œ ê³ ìœ  ì‹ë³„ì ì¶”ì¶œ íŒ¨í„´
        # [1], [2] ê°™ì€ ì°¸ì¡° í‘œì‹œë‚˜ "ì¶œì²˜: XXX"ì™€ ê°™ì€ íŒ¨í„´ ì°¾ê¸°
        ref_patterns = [
            r'\[(\d+)\]',  # [1], [2] ë“±ì˜ íŒ¨í„´
            r'ì¶œì²˜[:\s]+([^.\n]+)',  # "ì¶œì²˜: XXX" íŒ¨í„´
            r'ì°¸ê³ [:\s]+([^.\n]+)',  # "ì°¸ê³ : XXX" íŒ¨í„´
            r'ì°¸ì¡°[:\s]+([^.\n]+)',  # "ì°¸ì¡°: XXX" íŒ¨í„´
        ]
        
        # í…ìŠ¤íŠ¸ì—ì„œ ì‹¤ì œ ì‚¬ìš©ëœ ì°¸ì¡° ì‹ë³„
        used_refs = set()
        pattern_matches = []
        
        # ì°¸ì¡° ë²ˆí˜¸ íŒ¨í„´ ([1], [2] ë“±) ì°¾ê¸°
        print(Fore.CYAN + "\nğŸ” ì°¸ì¡° íŒ¨í„´ ê²€ìƒ‰ ê²°ê³¼:" + Style.RESET_ALL)
        for pattern in ref_patterns:
            matches = re.findall(pattern, response)
            if matches:
                pattern_matches.append((pattern, matches))
                print(f"  íŒ¨í„´ '{pattern}': {matches} ë°œê²¬")
                for match in matches:
                    try:
                        # ìˆ«ì íŒ¨í„´ì´ë©´ ì¸ë±ìŠ¤ë¡œ ì²˜ë¦¬
                        if match.isdigit():
                            ref_idx = int(match) - 1  # [1]ì€ ì¸ë±ìŠ¤ 0
                            if 0 <= ref_idx < len(original_references):
                                used_refs.add(ref_idx)
                                # print(f"    ì¸ë±ìŠ¤ {ref_idx} ({ref_idx+1}ë²ˆ ì°¸ì¡°) ì¶”ê°€ë¨")
                    except Exception as e:
                        print(f"    ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        
        if not pattern_matches:
            print("  ê°ì§€ëœ ì°¸ì¡° íŒ¨í„´ ì—†ìŒ")
        
        # ì œëª©ì´ë‚˜ URL ê¸°ë°˜ìœ¼ë¡œ ì§ì ‘ ì°¸ì¡°ëœ ê²ƒ ì°¾ê¸°
        print(Fore.CYAN + "\nğŸ” ë‚´ìš© ê¸°ë°˜ ì°¸ì¡° ê²€ìƒ‰:" + Style.RESET_ALL)
        content_matched = False
        
        for i, ref in enumerate(original_references):
            ref_value = ref.get('value', '').lower()
            ref_title = ref.get('title', '').lower()
            
            # ì°¸ì¡° URLì´ë‚˜ ì œëª©ì´ ì‘ë‹µì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            value_match = ref_value and ref_value in response.lower()
            title_match = ref_title and ref_title in response.lower()
            
            if value_match or title_match:
                used_refs.add(i)
                content_matched = True
                match_type = []
                if value_match: match_type.append("URL/ê°’")
                if title_match: match_type.append("ì œëª©")
                
                print(f"  {i+1}ë²ˆ ì°¸ì¡°: {', '.join(match_type)} ì¼ì¹˜ (ì¶”ê°€ë¨)")
        
        if not content_matched:
            print("  ë‚´ìš© ê¸°ë°˜ìœ¼ë¡œ ì¼ì¹˜í•˜ëŠ” ì°¸ì¡° ì—†ìŒ")
        
        # ì‚¬ìš©ëœ ì°¸ì¡° ëª©ë¡ ìƒì„±
        filtered_references = []
        
        # í…ìŠ¤íŠ¸ì— ì‹¤ì œ ì–¸ê¸‰ëœ ì°¸ì¡°ë§Œ ì¶”ê°€ (ì´ë¯¸ì§€ ìë™ í¬í•¨ X)
        for i in used_refs:
            if i < len(original_references):
                ref = original_references[i]
                if ref not in filtered_references:  # ì¤‘ë³µ ë°©ì§€
                    filtered_references.append(ref)
        
        print(Fore.CYAN + f"\nğŸ” ìµœì¢… ê²°ê³¼: ì›ë³¸ ì°¸ì¡° {len(original_references)}ê°œ ì¤‘ {len(filtered_references)}ê°œ í•„í„°ë§ë¨" + Style.RESET_ALL)
        if filtered_references:
            print("ğŸ“Œ ìµœì¢… ì„ íƒëœ ì°¸ì¡°:")
            for i, ref in enumerate(filtered_references):
                ref_type = ref.get('type', 'ì•Œ ìˆ˜ ì—†ìŒ')
                ref_title = ref.get('title', 'ì œëª© ì—†ìŒ')
                print(f"  {i+1}. [{ref_type}] {ref_title}")
        
        return filtered_references
    
    def _build_messages(self, state: AgentState):
        """
        Build messages for the LLM
        
        Args:
            state: The current agent state
            
        Returns:
            List of message objects for the LLM
        """
        context, references, original_references = self._build_context(state)
        
        # Format references as a newline-separated string
        references_text = "\n".join(references) if references else ""

        # ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ ì²˜ë¦¬
        conversation = []
        for msg in state.message_history:
            if msg["role"] == "user":
                conversation.append(HumanMessage(content=msg["content"]))
            elif msg["role"] == "assistant":
                conversation.append(AIMessage(content=msg["content"]))
        
        # í˜„ì¬ ì…ë ¥ ë©”ì‹œì§€ ì¶”ê°€
        conversation.append(HumanMessage(content=state.input))
        
        # Get messages from the prompt manager
        return prompt_manager.get_messages("responder",
            context=context,
            user_query=state.input,
            references=references_text,
            show_thought_process=self.show_thought_process,
            conversation=conversation
        )
    
    def _process_response(self, response: str) -> str:
        """
        Process the LLM response based on settings
        
        Args:
            response: Raw response from the LLM
            
        Returns:
            Processed response with thought process removed if needed
        """
        if not self.show_thought_process:
            # Remove thought process tags and content
            response = re.sub(r'<thought_process>.*?</thought_process>', '', response, flags=re.DOTALL)
            # Clean up excessive newlines
            response = re.sub(r'\n{3,}', '\n\n', response)
        
        return response.strip()
    
    async def __call__(self, state: AgentState):
        """
        Async generator for responding to queries
        
        Args:
            state: The current agent state
            
        Yields:
            Token chunks or final message with updated history
        """
        print(Fore.GREEN + "\nâœ¨ Generating final response" + Style.RESET_ALL)
        start_time = time.time()
        response = ""
        raw_response = ""

        # Get context and references before streaming
        context, references, original_references = self._build_context(state)
        
        if state.plan.direct_response:
            # Split the direct response into chunks and yield each chunk
            response = state.plan.direct_response
            raw_response = response
            chunk_size = 100  # Define the chunk size
            for i in range(0, len(response), chunk_size):
                chunk = response[i:i + chunk_size]
                yield chunk
        else:
            messages = self._build_messages(state)
            
            # Stream the response
            async for chunk in self.model.astream(messages):
                if hasattr(chunk, 'content'):
                    if isinstance(chunk.content, list):
                        # Claude 3 format
                        for content_item in chunk.content:
                            if content_item.get('type') == 'text':
                                token = content_item.get('text', '')
                                if token:
                                    raw_response += token
                                    yield token
                    else:
                        token = chunk.content
                        if token:
                            raw_response += token
                            yield token
            
            # Process the final response to remove thought process if needed
            response = self._process_response(raw_response)
        
        # ìµœì¢… ì‘ë‹µì—ì„œ ì‹¤ì œ ì‚¬ìš©ëœ ì°¸ì¡°ë§Œ í•„í„°ë§
        filtered_references = self._filter_references_from_response(response, original_references)
        
        # Yield final message with history
        yield {
            'response': response,
            'raw_response': raw_response,
            'message_history': [*state.message_history, {
                "role": "assistant",
                "content": response
            }],
            'references': filtered_references  # í•„í„°ë§ëœ ì°¸ì¡° ì‚¬ìš©
        }
        
        elapsed_time = time.time() - start_time
        print(Fore.YELLOW + f"\nâ±ï¸ Response generation took: {elapsed_time:.2f} seconds" + Style.RESET_ALL)

    async def acall(self, state: AgentState) -> Dict[str, Any]:
        """
        Non-streaming execution with complete response
        
        Args:
            state: The current agent state
            
        Returns:
            Dict containing updated state with response
        """
        # Build context and get references
        context, references, original_references = self._build_context(state)
        
        # Generate response
        response = await self.model.ainvoke(self._build_messages(state))
        raw_result = cast(str, response.content)
        result = self._process_response(raw_result)
        
        # ìµœì¢… ì‘ë‹µì—ì„œ ì‹¤ì œ ì‚¬ìš©ëœ ì°¸ì¡°ë§Œ í•„í„°ë§
        filtered_references = self._filter_references_from_response(result, original_references)
        
        # Update message history
        updated_message_history = [*state.message_history, {
            "role": "assistant",
            "content": result
        }]
        
        # Return state with response and references
        return {
            **state.model_dump(),
            "response": result,
            "raw_response": raw_result,
            "references": filtered_references,  # í•„í„°ë§ëœ ì°¸ì¡° ì‚¬ìš©
            "message_history": updated_message_history
        }